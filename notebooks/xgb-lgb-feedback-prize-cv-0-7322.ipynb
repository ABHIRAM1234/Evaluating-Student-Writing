{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9df0d1f",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e0167440-e64f-45dd-89fd-961bba1bea34",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.021324,
     "end_time": "2022-03-14T18:19:30.169676",
     "exception": false,
     "start_time": "2022-03-14T18:19:30.148352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d44f44d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:30.215208Z",
     "iopub.status.busy": "2022-03-14T18:19:30.213697Z",
     "iopub.status.idle": "2022-03-14T18:19:30.227189Z",
     "shell.execute_reply": "2022-03-14T18:19:30.227671Z",
     "shell.execute_reply.started": "2022-03-08T18:13:20.443882Z"
    },
    "papermill": {
     "duration": 0.037991,
     "end_time": "2022-03-14T18:19:30.227915",
     "exception": false,
     "start_time": "2022-03-14T18:19:30.189924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fp-test78',\n",
       " 'seqclassifiers',\n",
       " 'py-bigbird-v26',\n",
       " 'feedback-prize-2021',\n",
       " 'whitespace',\n",
       " 'deberta-large',\n",
       " 'student-writing-7322',\n",
       " 'deberta-xlarge',\n",
       " 'fullensemble',\n",
       " 'pytorch-longformer-large',\n",
       " 'fp-test63',\n",
       " 'seqclassifiers-shujun',\n",
       " 'seqclassifiers6',\n",
       " 'boostedcache']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"/kaggle/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6a8974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:30.280922Z",
     "iopub.status.busy": "2022-03-14T18:19:30.280318Z",
     "iopub.status.idle": "2022-03-14T18:19:47.480688Z",
     "shell.execute_reply": "2022-03-14T18:19:47.481178Z",
     "shell.execute_reply.started": "2022-03-07T05:26:10.716256Z"
    },
    "papermill": {
     "duration": 17.232218,
     "end_time": "2022-03-14T18:19:47.481324",
     "exception": false,
     "start_time": "2022-03-14T18:19:30.249106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [18:19:38.101227] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:38.148620] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:38.546604] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:38.596436] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:38.787694] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:38.837078] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:38.924096] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:38.962002] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:39.252816] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:39.305292] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:39.668504] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:39.720228] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:39.942371] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:39.991321] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:40.093063] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:40.136586] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:40.219651] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:40.270860] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:40.724334] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:40.782733] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:41.471554] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:41.765180] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:41.775055] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:42.076573] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:42.106665] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:42.327588] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:42.336459] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:42.533543] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:42.542943] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:42.740184] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:43.113756] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:43.324244] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:43.335813] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:43.482249] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:43.489388] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:43.663067] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:43.671486] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:43.805649] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:44.484927] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:44.632395] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:44.850781] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:44.960317] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:44.965287] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:45.034542] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:45.708689] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:45.818633] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:46.015511] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:46.120995] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:46.610061] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:46.701670] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:46.792357] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:46.816327] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:46.832871] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:46.876463] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:46.886942] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:46.916525] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:46.955240] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:46.976297] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:46.984970] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:47.005334] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:47.163978] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:47.189291] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:47.201562] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:47.234191] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:47.246334] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:47.289226] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:47.314312] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:47.361642] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [18:19:47.434136] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [18:19:47.463161] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n"
     ]
    }
   ],
   "source": [
    "from cuml import ForestInference\n",
    "\n",
    "\n",
    "discourses = ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal']\n",
    "xgb_models, lgb_models = dict(), dict()\n",
    "\n",
    "ensemble_weights = {\"Rebuttal\": 0.65,\n",
    "                    \"Counterclaim\": 0.75,\n",
    "                    \"Concluding Statement\": 0.60,\n",
    "                    \"Claim\": 0.65,\n",
    "                    \"Evidence\": 0.60,\n",
    "                    \"Position\": 0.75,\n",
    "                    \"Lead\": 0.70\n",
    "                    }\n",
    "\n",
    "\n",
    "thresholds = {'Lead': 0.66,\n",
    " 'Position': 0.56,\n",
    " 'Evidence': 0.57,\n",
    " 'Claim': 0.54,\n",
    " 'Concluding Statement': 0.56,\n",
    " 'Counterclaim': 0.7,\n",
    " 'Rebuttal': 0.74}\n",
    "\n",
    "features_dict = {'Lead': [i for i in range(34)],\n",
    " 'Position': [i for i in range(34)],\n",
    " 'Evidence': [i for i in range(20)],\n",
    " 'Claim': [i for i in range(20)],\n",
    " 'Concluding Statement': [i for i in range(34)],\n",
    " 'Counterclaim': [i for i in range(17)] + [i for i in range(27, 34)],\n",
    " 'Rebuttal': [i for i in range(17)]}\n",
    "\n",
    "N_XGB_FOLDS = 5\n",
    "\n",
    "for d in discourses:\n",
    "    model_list = []\n",
    "    for f in range(N_XGB_FOLDS):\n",
    "        xgb_model = ForestInference.load(f\"../input/student-writing-7322/xgb_{d}_{f}.json\", output_class=True, model_type=\"xgboost_json\")\n",
    "        model_list.append(xgb_model)\n",
    "    xgb_models[d] = model_list\n",
    "    \n",
    "    model_list = []\n",
    "    for f in range(N_XGB_FOLDS):\n",
    "        lgb_model = ForestInference.load(f\"../input/student-writing-7322/lgb_{d}_{f}.txt\", output_class=True, model_type=\"lightgbm\")\n",
    "        model_list.append(lgb_model)\n",
    "    lgb_models[d] = model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e42d242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:47.538298Z",
     "iopub.status.busy": "2022-03-14T18:19:47.534278Z",
     "iopub.status.idle": "2022-03-14T18:19:47.540744Z",
     "shell.execute_reply": "2022-03-14T18:19:47.540301Z",
     "shell.execute_reply.started": "2022-03-07T05:26:36.110299Z"
    },
    "papermill": {
     "duration": 0.03472,
     "end_time": "2022-03-14T18:19:47.540859",
     "exception": false,
     "start_time": "2022-03-14T18:19:47.506139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tp_prob(testDs, disc_type):\n",
    "\n",
    "    if testDs.features.shape[0] == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    pred = np.mean([clf.predict_proba(testDs.features[:, features_dict[disc_type]].astype(\"float32\"))[:,1] for clf in xgb_models[disc_type]], axis=0)/2\n",
    "    pred += np.mean([clf.predict_proba(testDs.features[:, features_dict[disc_type]].astype(\"float32\"))[:, 1] for clf in lgb_models[disc_type]], axis=0)/2\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf1fa6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:47.597448Z",
     "iopub.status.busy": "2022-03-14T18:19:47.596604Z",
     "iopub.status.idle": "2022-03-14T18:19:47.599081Z",
     "shell.execute_reply": "2022-03-14T18:19:47.598636Z",
     "shell.execute_reply.started": "2022-03-01T23:20:19.886495Z"
    },
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "8e783988-827d-4a32-9402-61603b89347b",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.033406,
     "end_time": "2022-03-14T18:19:47.599202",
     "exception": false,
     "start_time": "2022-03-14T18:19:47.565796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# DECLARE HOW MANY GPUS YOU WISH TO USE. \n",
    "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = '../input/py-bigbird-v26'\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "LOAD_MODEL_FROM = '../input/fp-test78'\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK \n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE \n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "DOWNLOADED_MODEL_PATH = '../input/deberta-xlarge' \n",
    "\n",
    "\n",
    "# A cache of the BigBird predictions for the validation/sequence training set and the corresponding sequence dataset\n",
    "KAGGLE_CACHE = '../input/feedbackcache2'\n",
    "\n",
    "N_FEATURES=34\n",
    "\n",
    "TEST_PERCENT = None\n",
    "\n",
    "cache = 'cache'\n",
    "cacheExists = os.path.exists(cache)\n",
    "if not cacheExists:\n",
    "  os.makedirs(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df76676c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:47.655347Z",
     "iopub.status.busy": "2022-03-14T18:19:47.654716Z",
     "iopub.status.idle": "2022-03-14T18:19:48.857860Z",
     "shell.execute_reply": "2022-03-14T18:19:48.857320Z",
     "shell.execute_reply.started": "2022-03-01T23:20:19.893966Z"
    },
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "42571122-fde4-4d21-959f-2bd6327e8741",
     "kernelId": ""
    },
    "papermill": {
     "duration": 1.233389,
     "end_time": "2022-03-14T18:19:48.857992",
     "exception": false,
     "start_time": "2022-03-14T18:19:47.624603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "config = {'model_name': '',   \n",
    "         'max_length': 2048,\n",
    "         'train_batch_size':4,\n",
    "         'valid_batch_size':4,\n",
    "         'epochs':5,\n",
    "         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "         'max_grad_norm':10,\n",
    "         'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e442467",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:48.915474Z",
     "iopub.status.busy": "2022-03-14T18:19:48.914907Z",
     "iopub.status.idle": "2022-03-14T18:19:57.371160Z",
     "shell.execute_reply": "2022-03-14T18:19:57.370652Z",
     "shell.execute_reply.started": "2022-03-01T23:20:21.357805Z"
    },
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "685d622c-38b1-4e57-99ff-c19b4d0e4b63",
     "kernelId": ""
    },
    "papermill": {
     "duration": 8.487609,
     "end_time": "2022-03-14T18:19:57.371282",
     "exception": false,
     "start_time": "2022-03-14T18:19:48.883673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np, os \n",
    "from scipy import stats\n",
    "import pandas as pd, gc \n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW\n",
    "from transformers import *\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.cuda import amp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', '.*__floordiv__ is deprecated.*',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecf56f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:57.430168Z",
     "iopub.status.busy": "2022-03-14T18:19:57.429510Z",
     "iopub.status.idle": "2022-03-14T18:19:57.474259Z",
     "shell.execute_reply": "2022-03-14T18:19:57.474667Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.555884Z"
    },
    "gradient": {
     "execution_count": 9,
     "id": "00c14a04-6f87-416f-a18f-2f50165da64a",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.078241,
     "end_time": "2022-03-14T18:19:57.474808",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.396567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>When people ask for advice,they sometimes talk...</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>During a group project, have you ever asked a ...</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Have you ever asked more than one person for h...</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>80% of Americans believe seeking multiple opin...</td>\n",
       "      <td>1056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            id                                               text  \\\n",
       "0      4  D46BCB48440A  When people ask for advice,they sometimes talk...   \n",
       "1      1  D72CB1C11673  Making choices in life can be very difficult. ...   \n",
       "2      0  0FB0700DAF44  During a group project, have you ever asked a ...   \n",
       "3      3  DF920E0A7337  Have you ever asked more than one person for h...   \n",
       "4      2  18409261F5C2  80% of Americans believe seeking multiple opin...   \n",
       "\n",
       "    len  \n",
       "0   363  \n",
       "1   421  \n",
       "2   635  \n",
       "3   711  \n",
       "4  1056  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "if TEST_PERCENT is not None:\n",
    "    print(f\"testing and submitting with only {TEST_PERCENT} of test data\")\n",
    "    np.random.seed(2022)\n",
    "    test_select=np.arange(len(test_texts))\n",
    "    np.random.shuffle(test_select)\n",
    "    test_texts=test_texts.iloc[test_select[:int(TEST_PERCENT*len(test_texts))]].reset_index()\n",
    "\n",
    "#sort by length of texts to minimize padding in each batch\n",
    "test_texts['len']=test_texts['text'].apply(lambda x:len(x.split()))\n",
    "test_texts=test_texts.sort_values(by=['len']).reset_index()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "test_texts\n",
    "\n",
    "SUBMISSION = True\n",
    "if len(test_names) > 5:\n",
    "      SUBMISSION = True\n",
    "\n",
    "test_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c6834",
   "metadata": {
    "papermill": {
     "duration": 0.024749,
     "end_time": "2022-03-14T18:19:57.525345",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.500596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert Train Text to NER Labels\n",
    "We will now convert all text words into NER labels and save in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e034c09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:57.582338Z",
     "iopub.status.busy": "2022-03-14T18:19:57.581840Z",
     "iopub.status.idle": "2022-03-14T18:19:57.585554Z",
     "shell.execute_reply": "2022-03-14T18:19:57.585120Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.609872Z"
    },
    "gradient": {
     "execution_count": 12,
     "id": "ee648d66-852f-47a5-a404-5890cde5c054",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.034692,
     "end_time": "2022-03-14T18:19:57.585664",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.550972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels)}\n",
    "disc_type_to_ids = {'Evidence':(11,12),'Claim':(5,6),'Lead':(1,2),'Position':(3,4),'Counterclaim':(7,8),'Rebuttal':(9,10),'Concluding Statement':(13,14)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34f91c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:57.641566Z",
     "iopub.status.busy": "2022-03-14T18:19:57.640913Z",
     "iopub.status.idle": "2022-03-14T18:19:57.643524Z",
     "shell.execute_reply": "2022-03-14T18:19:57.643916Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.618074Z"
    },
    "gradient": {
     "execution_count": 13,
     "id": "181002ad-1436-4890-8230-149cd4e7fcc1",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.033402,
     "end_time": "2022-03-14T18:19:57.644038",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.610636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-Lead': 1,\n",
       " 'I-Lead': 2,\n",
       " 'B-Position': 3,\n",
       " 'I-Position': 4,\n",
       " 'B-Claim': 5,\n",
       " 'I-Claim': 6,\n",
       " 'B-Counterclaim': 7,\n",
       " 'I-Counterclaim': 8,\n",
       " 'B-Rebuttal': 9,\n",
       " 'I-Rebuttal': 10,\n",
       " 'B-Evidence': 11,\n",
       " 'I-Evidence': 12,\n",
       " 'B-Concluding Statement': 13,\n",
       " 'I-Concluding Statement': 14}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cce5be",
   "metadata": {
    "papermill": {
     "duration": 0.02751,
     "end_time": "2022-03-14T18:19:57.697720",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.670210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the dataset function\n",
    "Below is our PyTorch dataset function. It always outputs tokens and attention. During training it also provides labels. And during inference it also provides word ids to help convert token predictions into word predictions.\n",
    "\n",
    "Note that we use `text.split()` and `is_split_into_words=True` when we convert train text to labeled train tokens. This is how the HugglingFace tutorial does it. However, this removes characters like `\\n` new paragraph. If you want your model to see new paragraphs, then we need to map words to tokens ourselves using `return_offsets_mapping=True`. See my TensorFlow notebook [here][1] for an example.\n",
    "\n",
    "Some of the following code comes from the example at HuggingFace [here][2]. However I think the code at that link is wrong. The HuggingFace original code is [here][3]. With the flag `LABEL_ALL` we can either label just the first subword token (when one word has more than one subword token). Or we can label all the subword tokens (with the word's label). In this notebook version, we label all the tokens. There is a Kaggle discussion [here][4]\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n",
    "[2]: https://huggingface.co/docs/transformers/custom_datasets#tok_ner\n",
    "[3]: https://github.com/huggingface/transformers/blob/86b40073e9aee6959c8c85fcba89e47b432c4f4d/examples/pytorch/token-classification/run_ner.py#L371\n",
    "[4]: https://www.kaggle.com/c/feedback-prize-2021/discussion/296713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696a6cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:57.755870Z",
     "iopub.status.busy": "2022-03-14T18:19:57.755279Z",
     "iopub.status.idle": "2022-03-14T18:19:57.758367Z",
     "shell.execute_reply": "2022-03-14T18:19:57.757953Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.634056Z"
    },
    "gradient": {
     "execution_count": 14,
     "id": "24ebb3cc-c13f-4356-b3b4-6e9dba4727ff",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.034539,
     "end_time": "2022-03-14T18:19:57.758497",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.723958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return an array that maps character index to index of word in list of split() words\n",
    "def split_mapping(unsplit):\n",
    "    splt = unsplit.split()\n",
    "    offset_to_wordidx = np.full(len(unsplit),-1)\n",
    "    txt_ptr = 0\n",
    "    for split_index, full_word in enumerate(splt):\n",
    "        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n",
    "            txt_ptr += 1\n",
    "        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n",
    "        txt_ptr += len(full_word)\n",
    "    return offset_to_wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddcd954c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:57.839049Z",
     "iopub.status.busy": "2022-03-14T18:19:57.838127Z",
     "iopub.status.idle": "2022-03-14T18:19:57.839914Z",
     "shell.execute_reply": "2022-03-14T18:19:57.840389Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.642568Z"
    },
    "gradient": {
     "execution_count": 15,
     "id": "7c2ec5a9-c120-4883-934d-41a83e3da1cc",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.056556,
     "end_time": "2022-03-14T18:19:57.840553",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.783997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids # for validation\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text,\n",
    "                             return_offsets_mapping=True, \n",
    "                             padding=False, \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        word_ids = encoding.word_ids()  \n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "        \n",
    "        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "        label_ids = []\n",
    "        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "            \n",
    "            if word_idx is None:\n",
    "                if not self.get_wids: label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx][0] != offsets[token_idx][1]:\n",
    "                    #Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                    \n",
    "                    if split_index != -1: \n",
    "                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if not self.get_wids: label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if not self.get_wids: label_ids.append(-100)\n",
    "                else:\n",
    "                    if not self.get_wids: label_ids.append(-100)\n",
    "        \n",
    "        encoding['labels'] = list(reversed(label_ids))\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids: \n",
    "            item['wids'] = torch.as_tensor(split_word_ids)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class CustomCollate:\n",
    "    def __init__(self,tokenizer,sliding_window=None):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.sliding_window=sliding_window\n",
    "\n",
    "    def __call__(self,data):\n",
    "        \"\"\"\n",
    "        need to collate: input_ids, attention_mask, labels\n",
    "        input_ids is padded with 1, attention_mask 0, labels -100\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        bs=len(data)\n",
    "        lengths=[]\n",
    "        for i in range(bs):\n",
    "            lengths.append(len(data[i]['input_ids']))\n",
    "            # print(data[i]['input_ids'].shape)\n",
    "            # print(data[i]['attention_mask'].shape)\n",
    "            # print(data[i]['labels'].shape)\n",
    "        max_len=max(lengths)\n",
    "        if self.sliding_window is not None and max_len > self.sliding_window:\n",
    "            max_len= int((np.floor(max_len/self.sliding_window-1e-6)+1)*self.sliding_window)\n",
    "        #always pad the right side\n",
    "        input_ids, attention_mask, labels, BIO_labels, discourse_labels=[],[],[],[],[]\n",
    "        #if np.random.uniform()>0.5:\n",
    "        #print(data[0].keys())\n",
    "        #print(max_len)\n",
    "        if 'wids' in data[0]:\n",
    "            get_wids=True\n",
    "        else:\n",
    "            get_wids=False\n",
    "        #print(get_wids)\n",
    "        wids = []\n",
    "            #wids.append(torch.nn.functional.pad(data[i]['wids'],(0,max_len-lengths[i]),value=-1))\n",
    "        for i in range(bs):\n",
    "            input_ids.append(torch.nn.functional.pad(data[i]['input_ids'],(0,max_len-lengths[i]),value=self.tokenizer.pad_token_id))\n",
    "            attention_mask.append(torch.nn.functional.pad(data[i]['attention_mask'],(0,max_len-lengths[i]),value=0))\n",
    "            #labels.append(torch.nn.functional.pad(data[i]['labels'],(0,max_len-lengths[i]),value=-100))\n",
    "            #BIO_labels.append(torch.nn.functional.pad(data[i]['BIO_labels'],(0,max_len-lengths[i]),value=-100))\n",
    "            #discourse_labels.append(torch.nn.functional.pad(data[i]['discourse_labels'],(0,max_len-lengths[i]),value=-100))\n",
    "            if get_wids:\n",
    "                wids.append(torch.nn.functional.pad(data[i]['wids'],(0,max_len-lengths[i]),value=-1))\n",
    "        # else:\n",
    "        #     for i in range(bs):\n",
    "        #         input_ids.append(torch.nn.functional.pad(data[i]['input_ids'],(max_len-lengths[i],0),value=1))\n",
    "        #         attention_mask.append(torch.nn.functional.pad(data[i]['attention_mask'],(max_len-lengths[i],0),value=0))\n",
    "        #         labels.append(torch.nn.functional.pad(data[i]['labels'],(max_len-lengths[i],0),value=-100))\n",
    "\n",
    "        input_ids=torch.stack(input_ids)\n",
    "        attention_mask=torch.stack(attention_mask)\n",
    "        #labels=torch.stack(labels)\n",
    "        #BIO_labels=torch.stack(BIO_labels)\n",
    "        #discourse_labels=torch.stack(discourse_labels)\n",
    "        if get_wids:\n",
    "            wids=torch.stack(wids)\n",
    "        #exit()\n",
    "        if get_wids:\n",
    "            return {\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
    "            \"labels\":labels,\"BIO_labels\":BIO_labels,\"discourse_labels\":discourse_labels,\n",
    "            \"wids\":wids}\n",
    "        else:\n",
    "            return {\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
    "            \"labels\":labels,\"BIO_labels\":BIO_labels,\"discourse_labels\":discourse_labels}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740b7a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:57.899132Z",
     "iopub.status.busy": "2022-03-14T18:19:57.898491Z",
     "iopub.status.idle": "2022-03-14T18:19:58.170393Z",
     "shell.execute_reply": "2022-03-14T18:19:58.169798Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.671898Z"
    },
    "gradient": {
     "execution_count": 18,
     "id": "3b670879-0d5d-44c6-950d-780518c2b3bf",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.303517,
     "end_time": "2022-03-14T18:19:58.170561",
     "exception": false,
     "start_time": "2022-03-14T18:19:57.867044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_params = {'batch_size': config['valid_batch_size'],\n",
    "                'shuffle': False,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \n",
    "\n",
    "\n",
    "# TEST DATASET\n",
    "test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "test_texts_loader = DataLoader(test_texts_set, **test_params,collate_fn=CustomCollate(tokenizer,512))\n",
    "\n",
    "tokenizer_longformer = AutoTokenizer.from_pretrained(\"../input/pytorch-longformer-large\") \n",
    "test_texts_set_longformer = dataset(test_texts, tokenizer_longformer, config['max_length'], True)\n",
    "test_texts_loader_longformer = DataLoader(test_texts_set_longformer, **test_params,collate_fn=CustomCollate(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246858a5",
   "metadata": {
    "papermill": {
     "duration": 0.025346,
     "end_time": "2022-03-14T18:19:58.226872",
     "exception": false,
     "start_time": "2022-03-14T18:19:58.201526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6547153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:58.313054Z",
     "iopub.status.busy": "2022-03-14T18:19:58.312358Z",
     "iopub.status.idle": "2022-03-14T18:19:58.315319Z",
     "shell.execute_reply": "2022-03-14T18:19:58.314889Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.937904Z"
    },
    "papermill": {
     "duration": 0.062669,
     "end_time": "2022-03-14T18:19:58.315450",
     "exception": false,
     "start_time": "2022-03-14T18:19:58.252781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "rearrange_indices=[14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "class ResidualLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,rnn):\n",
    "        super(ResidualLSTM, self).__init__()\n",
    "        self.downsample=nn.Linear(d_model,d_model//2)\n",
    "        if rnn=='GRU':\n",
    "            self.LSTM=nn.GRU(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n",
    "        else:\n",
    "            self.LSTM=nn.LSTM(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.norm1= nn.LayerNorm(d_model//2)\n",
    "        self.linear1=nn.Linear(d_model//2, d_model*4)\n",
    "        self.linear2=nn.Linear(d_model*4, d_model)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.norm2= nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res=x\n",
    "        x=self.downsample(x)\n",
    "        x, _ = self.LSTM(x)\n",
    "        x=self.dropout1(x)\n",
    "        x=self.norm1(x)\n",
    "        x=F.relu(self.linear1(x))\n",
    "        x=self.linear2(x)\n",
    "        x=self.dropout2(x)\n",
    "        x=res+x\n",
    "        return self.norm2(x)\n",
    "\n",
    "\n",
    "class ConvLSTMHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTMHead, self).__init__()\n",
    "        self.downsample=nn.Sequential(nn.Linear(1024,256))\n",
    "        self.conv1=  nn.Sequential(nn.Conv1d(256,256,3,padding=1),\n",
    "                                  nn.ReLU())\n",
    "        self.norm1 = nn.LayerNorm(256)\n",
    "        self.conv2=  nn.Sequential(nn.Conv1d(256,256,3,padding=1),\n",
    "                                  nn.ReLU())\n",
    "        self.norm2 = nn.LayerNorm(256)\n",
    "        #self.lstm=nn.LSTM(256,256,2,bidirectional=True)\n",
    "        self.lstm=ResidualLSTM(256)\n",
    "        self.upsample=nn.Sequential(nn.Linear(256,1024),nn.ReLU())\n",
    "        self.classification_head=nn.Sequential(nn.Linear(1024,15))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.downsample(x)\n",
    "        res=x\n",
    "        x=self.conv1(x.permute(0,2,1))\n",
    "        x=self.norm1(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x=self.conv2(x)\n",
    "        x=self.norm1(x.permute(0,2,1))\n",
    "        x=x+res\n",
    "        x=self.lstm(x.permute(1,0,2))\n",
    "        x=x.permute(1,0,2)\n",
    "        x=self.upsample(x)\n",
    "        x=self.classification_head(x)\n",
    "        #print(x.shape)\n",
    "        #exit()\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,DOWNLOADED_MODEL_PATH, rnn='LSTM'):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json')\n",
    "\n",
    "        self.backbone=AutoModel.from_pretrained(\n",
    "                           DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "\n",
    "        self.lstm=ResidualLSTM(1024,rnn)\n",
    "        self.classification_head=nn.Linear(1024,15)\n",
    "        #self.head=nn.Sequential(nn.Linear(1024,15))\n",
    "\n",
    "        # self.downsample=nn.Sequential(nn.Linear(1024,256))\n",
    "        # self.conv1d=nn.Sequential(nn.Conv1d(256,256,3,padding=0),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256),\n",
    "        #                           nn.Conv1d(256,256,3,padding=1),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256))\n",
    "\n",
    "        #self.BIO_head=nn.Sequential(nn.Linear(1024,3))\n",
    "\n",
    "    def forward(self,x,attention_mask):\n",
    "        x=self.backbone(input_ids=x,attention_mask=attention_mask,return_dict=False)[0]\n",
    "\n",
    "        x=self.lstm(x.permute(1,0,2)).permute(1,0,2)\n",
    "        x=self.classification_head(x)\n",
    "        # x=x.permute(0,2,1)\n",
    "        # x=self.conv1d(x)\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        # classification_output=self.classification_head(x)\n",
    "        #BIO_output=self.BIO_head(x[0])\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        return [x[:,:,rearrange_indices]]#, BIO_output\n",
    "    \n",
    "class SlidingWindowTransformerModel(nn.Module):\n",
    "    def __init__(self,DOWNLOADED_MODEL_PATH, rnn, window_size=512, edge_len=64):\n",
    "        super(SlidingWindowTransformerModel, self).__init__()\n",
    "        config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json')\n",
    "\n",
    "        self.backbone=AutoModel.from_pretrained(\n",
    "                           DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "\n",
    "        self.lstm=ResidualLSTM(1024,rnn)\n",
    "        self.classification_head=nn.Linear(1024,15)\n",
    "        self.window_size=window_size\n",
    "        self.edge_len=edge_len\n",
    "        self.inner_len=window_size-edge_len*2\n",
    "        #self.head=nn.Sequential(nn.Linear(1024,15))\n",
    "\n",
    "        # self.downsample=nn.Sequential(nn.Linear(1024,256))\n",
    "        # self.conv1d=nn.Sequential(nn.Conv1d(256,256,3,padding=0),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256),\n",
    "        #                           nn.Conv1d(256,256,3,padding=1),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256))\n",
    "\n",
    "        #self.BIO_head=nn.Sequential(nn.Linear(1024,3))\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "\n",
    "        B,L=input_ids.shape\n",
    "\n",
    "        # print(L)\n",
    "        # exit()\n",
    "        #x=self.backbone(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)[0]\n",
    "        if L<=self.window_size:\n",
    "            x=self.backbone(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)[0]\n",
    "            #pass\n",
    "        else:\n",
    "            #print(\"####\")\n",
    "            #print(input_ids.shape)\n",
    "            segments=(L-self.window_size)//self.inner_len\n",
    "            if (L-self.window_size)%self.inner_len>self.edge_len:\n",
    "                segments+=1\n",
    "            elif segments==0:\n",
    "                segments+=1\n",
    "            x=self.backbone(input_ids=input_ids[:,:self.window_size],attention_mask=attention_mask[:,:self.window_size],return_dict=False)[0]\n",
    "            for i in range(1,segments+1):\n",
    "                start=self.window_size-self.edge_len+(i-1)*self.inner_len\n",
    "                end=self.window_size-self.edge_len+(i-1)*self.inner_len+self.window_size\n",
    "                end=min(end,L)\n",
    "                x_next=input_ids[:,start:end]\n",
    "                mask_next=attention_mask[:,start:end]\n",
    "                x_next=self.backbone(input_ids=x_next,attention_mask=mask_next,return_dict=False)[0]\n",
    "                #L_next=x_next.shape[1]-self.edge_len,\n",
    "                if i==segments:\n",
    "                    x_next=x_next[:,self.edge_len:]\n",
    "                else:\n",
    "                    x_next=x_next[:,self.edge_len:self.edge_len+self.inner_len]\n",
    "                #print(x_next.shape)\n",
    "                x=torch.cat([x,x_next],1)\n",
    "\n",
    "                #print(start,end)\n",
    "        #print(x.shape)\n",
    "        x=self.lstm(x.permute(1,0,2)).permute(1,0,2)\n",
    "        x=self.classification_head(x)\n",
    "\n",
    "        # x=x.permute(0,2,1)\n",
    "        # x=self.conv1d(x)\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        # classification_output=self.classification_head(x)\n",
    "        #BIO_output=self.BIO_head(x[0])\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        #return x\n",
    "        return [x[:,:,rearrange_indices]]#, BIO_output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3b075",
   "metadata": {
    "papermill": {
     "duration": 0.02523,
     "end_time": "2022-03-14T18:19:58.366226",
     "exception": false,
     "start_time": "2022-03-14T18:19:58.340996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf210b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:58.432518Z",
     "iopub.status.busy": "2022-03-14T18:19:58.431619Z",
     "iopub.status.idle": "2022-03-14T18:19:58.433389Z",
     "shell.execute_reply": "2022-03-14T18:19:58.433893Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.970761Z"
    },
    "gradient": {
     "execution_count": 22,
     "id": "93df9ccc-f7ff-4cb2-8e20-8030026fff42",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.042335,
     "end_time": "2022-03-14T18:19:58.434020",
     "exception": false,
     "start_time": "2022-03-14T18:19:58.391685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Returns per-word, mean class prediction probability over all tokens corresponding to each word\n",
    "def inference(data_loader, model_ids, model, path):\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ensemble_preds = np.zeros((len(data_loader.dataset), config['max_length'], len(labels_to_ids)), dtype=np.float32)\n",
    "    wids = np.full((len(data_loader.dataset), config['max_length']), -100)\n",
    "    for model_i, model_id in enumerate(model_ids):\n",
    "        \n",
    "        model.load_state_dict(torch.load(f'{path}/fold{model_id}.pt', map_location=config['device']))\n",
    "        \n",
    "        # put model in training mode\n",
    "        model.eval()\n",
    "        for batch_i, batch in tqdm(enumerate(data_loader)):\n",
    "            \n",
    "            if model_i == 0: wids[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size'],:batch['wids'].shape[1]] = batch['wids'].numpy()\n",
    "\n",
    "            # MOVE BATCH TO GPU AND INFER\n",
    "            ids = batch[\"input_ids\"].to(config['device'])\n",
    "            mask = batch[\"attention_mask\"].to(config['device'])\n",
    "            with torch.no_grad():\n",
    "                #with amp.autocast():\n",
    "                outputs = model(ids, attention_mask=mask)\n",
    "            all_preds = torch.nn.functional.softmax(outputs[0], dim=2).cpu().detach().numpy() \n",
    "            ensemble_preds[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size'],:all_preds.shape[1]] += all_preds\n",
    "            \n",
    "            del ids\n",
    "            del mask\n",
    "            del outputs\n",
    "            del all_preds\n",
    "            \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    ensemble_preds /= len(model_ids)\n",
    "    predictions = []\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    for text_i in range(ensemble_preds.shape[0]):\n",
    "        token_preds = ensemble_preds[text_i]\n",
    "        \n",
    "        prediction = []\n",
    "        previous_word_idx = -1\n",
    "        prob_buffer = []\n",
    "        word_ids = wids[text_i][wids[text_i] != -100]\n",
    "        for idx,word_idx in enumerate(word_ids):                            \n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:              \n",
    "                if prob_buffer:\n",
    "                    prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "                    prob_buffer = []\n",
    "                prob_buffer.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "            else: \n",
    "                prob_buffer.append(token_preds[idx])\n",
    "        prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "        predictions.append(prediction)\n",
    "            \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99fa9686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:19:58.489337Z",
     "iopub.status.busy": "2022-03-14T18:19:58.488786Z",
     "iopub.status.idle": "2022-03-14T18:24:19.948660Z",
     "shell.execute_reply": "2022-03-14T18:24:19.949259Z",
     "shell.execute_reply.started": "2022-03-01T23:20:29.990181Z"
    },
    "gradient": {
     "execution_count": 24,
     "id": "7597203a-32ac-477b-951e-befcb89c709a",
     "kernelId": ""
    },
    "papermill": {
     "duration": 261.490198,
     "end_time": "2022-03-14T18:24:19.949472",
     "exception": false,
     "start_time": "2022-03-14T18:19:58.459274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-xlarge/pytorch_model.bin were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2it [00:04,  2.31s/it]\n",
      "2it [00:03,  1.68s/it]\n",
      "2it [00:03,  1.64s/it]\n",
      "2it [00:03,  1.64s/it]\n",
      "2it [00:03,  1.70s/it]\n",
      "2it [00:03,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "model = SlidingWindowTransformerModel(DOWNLOADED_MODEL_PATH,'GRU').to(config['device'])\n",
    "test_word_preds = inference(test_texts_loader, [0, 1, 3, 4, 6, 7], model, LOAD_MODEL_FROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "affdc684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:24:20.023123Z",
     "iopub.status.busy": "2022-03-14T18:24:20.022319Z",
     "iopub.status.idle": "2022-03-14T18:26:56.914640Z",
     "shell.execute_reply": "2022-03-14T18:26:56.915059Z",
     "shell.execute_reply.started": "2022-03-01T23:23:38.281Z"
    },
    "papermill": {
     "duration": 156.931873,
     "end_time": "2022-03-14T18:26:56.915228",
     "exception": false,
     "start_time": "2022-03-14T18:24:19.983355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/pytorch-longformer-large/pytorch_model.bin were not used when initializing LongformerModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at ../input/pytorch-longformer-large/pytorch_model.bin and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2it [00:01,  1.39it/s]\n",
      "2it [00:01,  1.30it/s]\n",
      "2it [00:01,  1.41it/s]\n",
      "2it [00:01,  1.44it/s]\n",
      "2it [00:01,  1.34it/s]\n",
      "2it [00:01,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(\"../input/pytorch-longformer-large\",'GRU').to(config['device'])\n",
    "test_word_preds2 = inference(test_texts_loader_longformer, [0, 2, 3, 4, 5, 6], model, \"../input/fp-test63\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3591d",
   "metadata": {
    "papermill": {
     "duration": 0.042569,
     "end_time": "2022-03-14T18:26:56.999828",
     "exception": false,
     "start_time": "2022-03-14T18:26:56.957259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence Datasets\n",
    "We will create datasets that, instead of describing individual words or tokens, describes sequences of words. Within some heuristic constraints, every possible sub-sequence of words in a text will converted to a dataset sample with the following attributes:\n",
    "\n",
    "* features- sequence length, position, and various kinds of class probability predictions/statistics\n",
    "* labels- whether the sequence matches exactly a discourse instance\n",
    "* truePos- whether the sequence matches a discourse instance by competition criteria for true positive\n",
    "* groups- the integer index of the text where the sequence is found\n",
    "* wordRanges- the start and end word index of the sequence in the text\n",
    "\n",
    "Sequence datasets are generated for each discourse type and for validation and submission datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12608064",
   "metadata": {
    "papermill": {
     "duration": 0.042139,
     "end_time": "2022-03-14T18:26:57.084005",
     "exception": false,
     "start_time": "2022-03-14T18:26:57.041866",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f035dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:26:57.176711Z",
     "iopub.status.busy": "2022-03-14T18:26:57.175797Z",
     "iopub.status.idle": "2022-03-14T18:26:59.144702Z",
     "shell.execute_reply": "2022-03-14T18:26:59.144181Z",
     "shell.execute_reply.started": "2022-03-01T23:26:51.633145Z"
    },
    "gradient": {
     "execution_count": 25,
     "id": "268caa30-cdb4-4dbe-873e-60d40637e003",
     "kernelId": ""
    },
    "papermill": {
     "duration": 2.019206,
     "end_time": "2022-03-14T18:26:59.144843",
     "exception": false,
     "start_time": "2022-03-14T18:26:57.125637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "# Percentile code taken from https://www.kaggle.com/vuxxxx/tensorflow-longformer-ner-postprocessing\n",
    "# Thank Vu!\n",
    "#\n",
    "# Use 99.5% of the distribution of lengths for a disourse type as maximum. \n",
    "# Increasing this constraint makes this step slower but generally increases performance.\n",
    "train_df=pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\n",
    "MAX_SEQ_LEN = {}\n",
    "train_df['len'] = train_df['predictionstring'].apply(lambda x:len(x.split()))\n",
    "max_lens = train_df.groupby('discourse_type')['len'].quantile(.995)\n",
    "for disc_type in disc_type_to_ids:\n",
    "    MAX_SEQ_LEN[disc_type] = int(max_lens[disc_type])\n",
    "\n",
    "#The minimum probability prediction for a 'B'egin class for which we will evaluate a word sequence\n",
    "MIN_BEGIN_PROB = {\n",
    "    'Claim': .35*0.8,\n",
    "    'Concluding Statement': .15*1.0,\n",
    "    'Counterclaim': .04*1.25,\n",
    "    'Evidence': .1*0.8,\n",
    "    'Lead': .32*1.0,\n",
    "    'Position': .25*0.8,\n",
    "    'Rebuttal': .01*1.25,\n",
    "}\n",
    "        \n",
    "class SeqDataset(object):\n",
    "    \n",
    "    def __init__(self, features, labels, groups, wordRanges, truePos):\n",
    "        \n",
    "        self.features = np.array(features, dtype=np.float32)\n",
    "        self.labels = np.array(labels)\n",
    "        self.groups = np.array(groups, dtype=np.int16)\n",
    "        self.wordRanges = np.array(wordRanges, dtype=np.int16)\n",
    "        self.truePos = np.array(truePos)\n",
    "\n",
    "# Adapted from https://stackoverflow.com/questions/60467081/linear-interpolation-in-numpy-quantile\n",
    "# This is used to prevent re-sorting to compute quantile for every sequence.\n",
    "def sorted_quantile(array, q):\n",
    "    array = np.array(array)\n",
    "    n = len(array)\n",
    "    index = (n - 1) * q\n",
    "    left = np.floor(index).astype(int)\n",
    "    fraction = index - left\n",
    "    right = left\n",
    "    right = right + (fraction > 0).astype(int)\n",
    "    i, j = array[left], array[right]\n",
    "    return i + (j - i) * fraction\n",
    "        \n",
    "def seq_dataset(disc_type, pred_indices=None, submit=False):\n",
    "    begin_class_ids = [0, 1, 3, 5, 7, 9, 11, 13]\n",
    "    word_preds = valid_word_preds if not submit else test_word_preds   \n",
    "    w = ensemble_weights[disc_type]\n",
    "    \n",
    "    \n",
    "    window = pred_indices if pred_indices else range(len(word_preds))\n",
    "    X = np.empty((int(1e6),N_FEATURES), dtype=np.float32)\n",
    "    X_ind = 0\n",
    "    y = []\n",
    "    truePos = []\n",
    "    wordRanges = []\n",
    "    groups = []\n",
    "    for text_i in window:\n",
    "        text_preds, text_preds2 = np.array(test_word_preds[text_i]), np.array(test_word_preds2[text_i])\n",
    "        \n",
    "        if len(text_preds) <= len(text_preds2):\n",
    "            text_preds = w*text_preds + (1-w)*text_preds2[:len(text_preds)] \n",
    "        else:\n",
    "            text_preds[:len(text_preds2)] = w*text_preds[:len(text_preds2)] + (1-w)*text_preds2\n",
    "        \n",
    "        num_words = len(text_preds)\n",
    "        \n",
    "        global_features, global_locs = [], []\n",
    "        \n",
    "        for dt in disc_type_to_ids:\n",
    "            disc_begin, disc_inside = disc_type_to_ids[dt]\n",
    "            \n",
    "            gmean = (text_preds[:, disc_begin] + text_preds[:, disc_inside]).mean()\n",
    "            global_features.append(gmean)\n",
    "            global_locs.append(np.argmax(text_preds[:, disc_begin])/float(num_words))\n",
    "        \n",
    "        disc_begin, disc_inside = disc_type_to_ids[disc_type]\n",
    "        \n",
    "        # The probability that a word corresponds to either a 'B'-egin or 'I'-nside token for a class\n",
    "        prob_or = lambda word_preds: word_preds[:,disc_begin] + word_preds[:,disc_inside]\n",
    "        \n",
    "        if not submit:\n",
    "            gt_idx = set()\n",
    "            gt_arr = np.zeros(num_words, dtype=int)\n",
    "            text_gt = valid.loc[valid.id == test_dataset.id.values[text_i]]\n",
    "            disc_gt = text_gt.loc[text_gt.discourse_type == disc_type]\n",
    "            \n",
    "            # Represent the discourse instance locations in a hash set and an integer array for speed\n",
    "            for row_i, row in enumerate(disc_gt.iterrows()):\n",
    "                splt = row[1]['predictionstring'].split()\n",
    "                start, end = int(splt[0]), int(splt[-1]) + 1\n",
    "                gt_idx.add((start, end))\n",
    "                gt_arr[start:end] = row_i + 1\n",
    "            gt_lens = np.bincount(gt_arr)\n",
    "        \n",
    "        # Iterate over every sub-sequence in the text\n",
    "        quants = np.linspace(0,1,7)\n",
    "        prob_begins = np.copy(text_preds[:,disc_begin])\n",
    "        min_begin = MIN_BEGIN_PROB[disc_type]\n",
    "        for pred_start in range(num_words):\n",
    "            prob_begin = prob_begins[pred_start]\n",
    "            if prob_begin > min_begin:\n",
    "                begin_or_inside = []\n",
    "                for pred_end in range(pred_start+1,min(num_words+1, pred_start+MAX_SEQ_LEN[disc_type]+1)):\n",
    "                    \n",
    "                    new_prob = prob_or(text_preds[pred_end-1:pred_end])\n",
    "                    insert_i = bisect_left(begin_or_inside, new_prob)\n",
    "                    begin_or_inside.insert(insert_i, new_prob[0])\n",
    "\n",
    "                    # Generate features for a word sub-sequence\n",
    "\n",
    "                    # The length and position of start/end of the sequence\n",
    "                    features = [pred_end - pred_start, pred_start / float(num_words), pred_end / float(num_words)]\n",
    "                    \n",
    "                    # 7 evenly spaced quantiles of the distribution of relevant class probabilities for this sequence\n",
    "                    features.extend(list(sorted_quantile(begin_or_inside, quants)))\n",
    "\n",
    "                    # The probability that words on either edge of the current sub-sequence belong to the class of interest\n",
    "                    features.append(prob_or(text_preds[pred_start-1:pred_start])[0] if pred_start > 0 else 0)\n",
    "                    features.append(prob_or(text_preds[pred_end:pred_end+1])[0] if pred_end < num_words else 0)\n",
    "                    features.append(prob_or(text_preds[pred_start-2:pred_start-1])[0] if pred_start > 1 else 0)\n",
    "                    features.append(prob_or(text_preds[pred_end+1:pred_end+2])[0] if pred_end < (num_words-1) else 0)\n",
    "                    \n",
    "                    # The probability that the first word corresponds to a 'B'-egin token\n",
    "                    features.append(text_preds[pred_start,disc_begin])\n",
    "                    features.append(text_preds[pred_start-1,disc_begin])\n",
    "                    \n",
    "                    if pred_end < num_words:\n",
    "                        features.append(text_preds[pred_end, begin_class_ids].sum())\n",
    "                    else:\n",
    "                        features.append(1.0)\n",
    "                    \n",
    "                    s = prob_or(text_preds[pred_start:pred_end])\n",
    "                    features.append(np.argmax(s)/features[0]) # maximum point location on sequence\n",
    "                    features.append(np.argmin(s)/features[0]) # minimum point location on sequence\n",
    "                    instability = 0\n",
    "                    if len(s) > 1:\n",
    "                        instability = (np.diff(s)**2).mean()\n",
    "                    features.append(instability)\n",
    "                    \n",
    "                    features.extend(list(global_features))\n",
    "                    features.extend(list([loc - features[1] for loc in global_locs]))\n",
    "                    \n",
    "                    exact_match = (pred_start, pred_end) in gt_idx if not submit else None\n",
    "\n",
    "                    if not submit:\n",
    "                        true_pos = False\n",
    "                        for match_cand, count in Counter(gt_arr[pred_start:pred_end]).most_common(2):\n",
    "                            if match_cand != 0 and count / float(pred_end - pred_start) >= .5 and float(count) / gt_lens[match_cand] >= .5: true_pos = True\n",
    "                    else: true_pos = None\n",
    "\n",
    "                    # For efficiency, use a numpy array instead of a list that doubles in size when full to conserve constant \"append\" time complexity\n",
    "                    if X_ind >= X.shape[0]:\n",
    "                        new_X = np.empty((X.shape[0]*2,N_FEATURES), dtype=np.float32)\n",
    "                        new_X[:X.shape[0]] = X\n",
    "                        X = new_X\n",
    "                    X[X_ind] = features\n",
    "                    X_ind += 1\n",
    "                    \n",
    "                    y.append(exact_match)\n",
    "                    truePos.append(true_pos)\n",
    "                    wordRanges.append((np.int16(pred_start), np.int16(pred_end)))\n",
    "                    groups.append(np.int16(text_i))\n",
    "\n",
    "    return SeqDataset(X[:X_ind], y, groups, wordRanges, truePos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8e26c",
   "metadata": {
    "papermill": {
     "duration": 0.043117,
     "end_time": "2022-03-14T18:26:59.231318",
     "exception": false,
     "start_time": "2022-03-14T18:26:59.188201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict strings and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "852103fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:26:59.344773Z",
     "iopub.status.busy": "2022-03-14T18:26:59.343908Z",
     "iopub.status.idle": "2022-03-14T18:26:59.547211Z",
     "shell.execute_reply": "2022-03-14T18:26:59.546539Z",
     "shell.execute_reply.started": "2022-03-01T23:32:52.197383Z"
    },
    "gradient": {
     "execution_count": 27,
     "id": "cc2e7c81-2347-4b3d-90c6-2b52d2ffa036",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.269066,
     "end_time": "2022-03-14T18:26:59.547350",
     "exception": false,
     "start_time": "2022-03-14T18:26:59.278284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "from sklearn.model_selection import cross_val_score, GroupKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from skopt.space import Real\n",
    "from skopt import gp_minimize\n",
    "import sys\n",
    "import xgboost\n",
    "\n",
    "NUM_FOLDS = 8\n",
    "\n",
    "warnings.filterwarnings('ignore', '.*ragged nested sequences*',)\n",
    "\n",
    "prob_cache = {} # Cache each fold's probability predictions for speed\n",
    "clfs = []  # Each fold will add its classifier here\n",
    "# Predict sub-sequences for a discourse type and set of train/test texts\n",
    "def predict_strings(disc_type, probThresh, test_groups, train_ind=None, submit=False):\n",
    "    string_preds = []\n",
    "    #validSeqDs = validSeqSets[disc_type]\n",
    "    #submitSeqDs = submitSeqSets[disc_type]\n",
    "    \n",
    "    # Average the probability predictions of a set of classifiers\n",
    "    \n",
    "\n",
    "    \n",
    "    predict_df = test_texts\n",
    "    text_df = test_texts\n",
    "    \n",
    "    for text_idx in tqdm(test_groups):\n",
    "        # The probability of true positive and (start,end) of each sub-sequence in the curent text\n",
    "        \n",
    "        testDs=seq_dataset(disc_type, pred_indices=[text_idx],submit=True)\n",
    "        \n",
    "        prob_tp_curr = get_tp_prob(testDs, disc_type)\n",
    "        word_ranges_curr = testDs.wordRanges[testDs.groups == text_idx]\n",
    "        \n",
    "        split_text = text_df.loc[text_df.id == predict_df.id.values[text_idx]].iloc[0].text.split()\n",
    "        full_preds = np.zeros(len(split_text))\n",
    "        # Include the sub-sequence predictions in order of predicted probability\n",
    "        for prob, wordRange in reversed(sorted(zip(prob_tp_curr, [tuple(wr) for wr in word_ranges_curr]))):\n",
    "            \n",
    "            # Until the predicted probability is lower than the tuned threshold\n",
    "            if prob < probThresh: break\n",
    "                \n",
    "            intersect = np.sum(full_preds[wordRange[0]:wordRange[1]])        \n",
    "            total = wordRange[1] - wordRange[0]\n",
    "            condition = intersect/total <= 0.15\n",
    "\n",
    "            if condition:\n",
    "                full_preds[wordRange[0]:wordRange[1]] = 1\n",
    "                string_preds.append((predict_df.id.values[text_idx], disc_type, ' '.join(map(str, list(range(wordRange[0], wordRange[1]))))))\n",
    "    return string_preds\n",
    "\n",
    "def sub_df(string_preds):\n",
    "    return pd.DataFrame(string_preds, columns=['id','class','predictionstring'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7220c59",
   "metadata": {
    "papermill": {
     "duration": 0.042182,
     "end_time": "2022-03-14T18:26:59.632268",
     "exception": false,
     "start_time": "2022-03-14T18:26:59.590086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load the tuned probability thresholds from tuning result files, and make sub-sequence predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38706436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:26:59.724415Z",
     "iopub.status.busy": "2022-03-14T18:26:59.723430Z",
     "iopub.status.idle": "2022-03-14T18:27:02.327743Z",
     "shell.execute_reply": "2022-03-14T18:27:02.328286Z",
     "shell.execute_reply.started": "2022-03-01T23:32:54.2975Z"
    },
    "gradient": {
     "execution_count": 28,
     "id": "087e600e-f15f-4154-8c26-6a8dabcfb73b",
     "kernelId": ""
    },
    "papermill": {
     "duration": 2.653054,
     "end_time": "2022-03-14T18:27:02.328493",
     "exception": false,
     "start_time": "2022-03-14T18:26:59.675439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.08it/s]\n",
      "100%|| 5/5 [00:00<00:00, 22.24it/s]\n",
      "100%|| 5/5 [00:00<00:00, 16.16it/s]\n",
      "100%|| 5/5 [00:00<00:00, 27.19it/s]\n",
      "100%|| 5/5 [00:00<00:00, 178.90it/s]\n",
      "100%|| 5/5 [00:00<00:00, 151.51it/s]\n",
      "100%|| 5/5 [00:00<00:00, 34.39it/s]\n"
     ]
    }
   ],
   "source": [
    "uniqueSubmitGroups = range(len(test_word_preds))\n",
    "\n",
    "sub = pd.concat([sub_df(predict_strings(disc_type, thresholds[disc_type], \n",
    "                                        uniqueSubmitGroups, submit=True)) for disc_type in disc_type_to_ids ]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1001bbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T18:27:02.443823Z",
     "iopub.status.busy": "2022-03-14T18:27:02.443117Z",
     "iopub.status.idle": "2022-03-14T18:27:02.455275Z",
     "shell.execute_reply": "2022-03-14T18:27:02.455683Z",
     "shell.execute_reply.started": "2022-03-01T23:26:54.447223Z"
    },
    "gradient": {
     "execution_count": 29,
     "id": "658e2ab3-b02d-449c-ad21-340e6206586f",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.068755,
     "end_time": "2022-03-14T18:27:02.455819",
     "exception": false,
     "start_time": "2022-03-14T18:27:02.387064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>150 151 152 153 154 155 156 157 158 159 160 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>223 224 225 226 227 228 229 230 231 232 233 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>183 184 185 186 187 188 189 190 191 192 193 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>274 275 276 277 278 279 280 281 282 283 284 28...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     class                                   predictionstring\n",
       "0  D46BCB48440A  Evidence  56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...\n",
       "1  D46BCB48440A  Evidence  150 151 152 153 154 155 156 157 158 159 160 16...\n",
       "2  D46BCB48440A  Evidence  223 224 225 226 227 228 229 230 231 232 233 23...\n",
       "3  D72CB1C11673  Evidence  183 184 185 186 187 188 189 190 191 192 193 19...\n",
       "4  D72CB1C11673  Evidence  274 275 276 277 278 279 280 281 282 283 284 28..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 464.677542,
   "end_time": "2022-03-14T18:27:05.785918",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-14T18:19:21.108376",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

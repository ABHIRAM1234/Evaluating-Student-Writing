{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711d21e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:40:49.473847Z",
     "iopub.status.busy": "2022-03-15T13:40:49.472332Z",
     "iopub.status.idle": "2022-03-15T13:47:43.666341Z",
     "shell.execute_reply": "2022-03-15T13:47:43.666851Z",
     "shell.execute_reply.started": "2022-03-13T05:24:56.501963Z"
    },
    "papermill": {
     "duration": 414.214196,
     "end_time": "2022-03-15T13:47:43.667149",
     "exception": false,
     "start_time": "2022-03-15T13:40:49.452953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log path: submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=10, fold=1, key_string='submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1', load_feat=True, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=512, model_name='../input/roberta-large/submission/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=4, train_padding_side='random', valid_batch_size=4)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/distilbart_xsum_12_6_adv_fold5.py文件执行成功！\n",
      "log path: submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=10, fold=1, key_string='submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1', load_feat=True, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=512, model_name='../input/roberta-large/submission/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=4, train_padding_side='random', valid_batch_size=4)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n",
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/distilbart_cnn_12_6_adv_fold5.py文件执行成功！\n",
      "log path: longformer-large-4096_v2_15class_padding_random_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=8, fold=1, key_string='longformer-large-4096_v2_15class_padding_random_fold1', load_feat=True, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=None, model_name='../input/longformerlarge4096/longformer-large-4096/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=8, train_padding_side='random', valid_batch_size=8)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n",
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/robert_fold5.py文件执行成功！\n",
      "log path: submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=10, fold=1, key_string='submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1', load_feat=False, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=512, model_name='../input/roberta-large/submission/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=4, train_padding_side='random', valid_batch_size=4)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/bart_large_finetuned_squadv1_fold5.py文件执行成功！\n",
      "log path: submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=10, fold=1, key_string='submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1', load_feat=True, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=512, model_name='../input/roberta-large/submission/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=4, train_padding_side='random', valid_batch_size=4)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n",
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/distilbart_mnli_12_9_adv_fold5.py文件执行成功！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 291.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/feedback-py/hexian.py文件执行成功！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log path: submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=10, fold=1, key_string='submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1', load_feat=False, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=512, model_name='../input/roberta-large/submission/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=4, train_padding_side='random', valid_batch_size=4)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n",
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:09,  4.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/debert_xxlarge_adv_fold5.py文件执行成功！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log path: submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=10, fold=1, key_string='submission_aistudio_15class_padding_random_max_lr_1e-05_max_length_4096_fold1', load_feat=False, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=512, model_name='../input/roberta-large/submission/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=4, train_padding_side='random', valid_batch_size=4)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n",
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/debert_xlarge_fold5.py文件执行成功！\n",
      "log path: longformer-large-4096_v2_15class_padding_random_fold1.log\n",
      "args:Namespace(adv_eps=0.001, adv_lr=0.0, cache_path='./', data_path='../input/feedback-prize-2021/', debug=False, device='cuda', epochs=8, fold=1, key_string='longformer-large-4096_v2_15class_padding_random_fold1', load_feat=True, load_model=False, log_path='log.txt', lr=1e-05, max_grad_norm=10, max_length=4096, max_lr=1e-05, min_lr=1e-06, model_length=None, model_name='../input/longformerlarge4096/longformer-large-4096/', padding_dict={'input_ids': 0, 'attention_mask': 0, 'labels': -100}, seed=66, test_padding_side='right', text_path='../input/feedback-prize-2021/test/', train_batch_size=8, train_padding_side='random', valid_batch_size=8)\n",
      "test_df.shape: (5, 3) \t\n",
      "开始生成特征...\n",
      "test_feat.shap: (5, 6) \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ../input/wenfeng-models-code4/longformer_fold5.py文件执行成功！\n"
     ]
    }
   ],
   "source": [
    "# !python ../input/feedback-py/hexian.py\n",
    "# !python ../input/wenfeng-models-code/debert_xxlarge_fold12.py\n",
    "# !python ../input/wenfeng-models-code/robert_fold34.py\n",
    "# !python ../input/wenfeng-models-code/distilbart_mnli_12_9_adv_fold02.py\n",
    "def run_cmd(cmd):\n",
    "    import os\n",
    "    a = os.system(cmd)\n",
    "    if a != 0:\n",
    "        raise Exception(cmd+\"文件执行错误\")\n",
    "    else:\n",
    "        print(cmd+\"文件执行成功！\")\n",
    "\n",
    "run_cmd('python ../input/wenfeng-models-code4/distilbart_xsum_12_6_adv_fold5.py')\n",
    "run_cmd('python ../input/wenfeng-models-code4/distilbart_cnn_12_6_adv_fold5.py')\n",
    "run_cmd('python ../input/wenfeng-models-code4/robert_fold5.py')\n",
    "run_cmd('python ../input/wenfeng-models-code4/bart_large_finetuned_squadv1_fold5.py')\n",
    "run_cmd('python ../input/wenfeng-models-code4/distilbart_mnli_12_9_adv_fold5.py')\n",
    "run_cmd('python ../input/feedback-py/hexian.py')\n",
    "run_cmd('python ../input/wenfeng-models-code4/debert_xxlarge_adv_fold5.py')\n",
    "run_cmd('python ../input/wenfeng-models-code4/debert_xlarge_fold5.py')\n",
    "# run_cmd('python ../input/wenfeng-models-code3/2longformer_2robert_2dist_2squadv1.py')\n",
    "run_cmd('python ../input/wenfeng-models-code4/longformer_fold5.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb116fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:47:43.779925Z",
     "iopub.status.busy": "2022-03-15T13:47:43.764539Z",
     "iopub.status.idle": "2022-03-15T13:48:11.356457Z",
     "shell.execute_reply": "2022-03-15T13:48:11.355200Z",
     "shell.execute_reply.started": "2022-03-13T05:30:59.0942Z"
    },
    "papermill": {
     "duration": 27.659923,
     "end_time": "2022-03-15T13:48:11.356647",
     "exception": false,
     "start_time": "2022-03-15T13:47:43.696724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "融合文件./longformer_fold5.pkl和./robert_fold5.pkl\r\n",
      "融合文件./ronghe.pkl和./debert_xxlarge_adv_fold5.pkl\r\n",
      "融合文件./ronghe.pkl和./hexian.pkl\r\n",
      "融合文件./ronghe.pkl和./distilbart_mnli_12_9_adv_fold5.pkl\r\n",
      "融合文件./ronghe.pkl和./bart_large_finetuned_squadv1_fold5.pkl\r\n",
      "融合文件./ronghe.pkl和./debert_xlarge_fold5.pkl\r\n",
      "融合文件./ronghe.pkl和./distilbart_cnn_12_6_adv_fold5.pkl\r\n",
      "融合文件./ronghe.pkl和./distilbart_xsum_12_6_adv_fold5.pkl\r\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(10)\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './longformer_fold5.pkl' \\\n",
    "--path2 './robert_fold5.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 0 \\\n",
    "--weight 1.5\n",
    "! rm ./longformer_fold5.pkl\n",
    "! rm ./robert_fold5.pkl\n",
    "\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './ronghe.pkl' \\\n",
    "--path2 './debert_xxlarge_adv_fold5.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 1 \\\n",
    "--weight 2.5\n",
    "! rm ./debert_xxlarge_adv_fold5.pkl\n",
    "\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './ronghe.pkl' \\\n",
    "--path2 './hexian.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 1 \\\n",
    "--weight 1.5\n",
    "! rm ./hexian.pkl\n",
    "\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './ronghe.pkl' \\\n",
    "--path2 './distilbart_mnli_12_9_adv_fold5.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 0 \\\n",
    "--weight 0.5\n",
    "! rm ./distilbart_mnli_12_9_adv_fold5.pkl\n",
    "\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './ronghe.pkl' \\\n",
    "--path2 './bart_large_finetuned_squadv1_fold5.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 0 \\\n",
    "--weight 0.5\n",
    "! rm ./bart_large_finetuned_squadv1_fold5.pkl\n",
    "\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './ronghe.pkl' \\\n",
    "--path2 './debert_xlarge_fold5.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 1 \\\n",
    "--weight 0.5\n",
    "! rm ./debert_xlarge_fold5.pkl\n",
    "\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './ronghe.pkl' \\\n",
    "--path2 './distilbart_cnn_12_6_adv_fold5.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 0 \\\n",
    "--weight 0.5\n",
    "! rm ./distilbart_cnn_12_6_adv_fold5.pkl\n",
    "\n",
    "! python ../input/wenfeng-models-code3/pred_add.py \\\n",
    "--path1 './ronghe.pkl' \\\n",
    "--path2 './distilbart_xsum_12_6_adv_fold5.pkl' \\\n",
    "--path3 './ronghe.pkl' \\\n",
    "--align 0 \\\n",
    "--weight 0.5\n",
    "! rm ./distilbart_xsum_12_6_adv_fold5.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fb19ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:11.465534Z",
     "iopub.status.busy": "2022-03-15T13:48:11.464679Z",
     "iopub.status.idle": "2022-03-15T13:48:11.467145Z",
     "shell.execute_reply": "2022-03-15T13:48:11.467734Z",
     "shell.execute_reply.started": "2022-03-13T05:31:17.457126Z"
    },
    "papermill": {
     "duration": 0.060453,
     "end_time": "2022-03-15T13:48:11.468064",
     "exception": false,
     "start_time": "2022-03-15T13:48:11.407611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import sys\n",
    "# import pickle\n",
    "# sys.path.append('../input/wenfeng-models-code2')\n",
    "# from utils import after_deal\n",
    "\n",
    "\n",
    "# test_feat = pickle.load(open('./ronghe.pkl','+rb'))\n",
    "# test_feat['pred'] = test_feat['pred'].apply(lambda x: x/x.sum(axis=1).reshape(-1,1))  # 重要不要遗漏\n",
    "\n",
    "\n",
    "# discourse_type = ['Claim','Evidence', 'Position','Concluding Statement','Lead','Counterclaim','Rebuttal']\n",
    "# i_discourse_type = ['I-'+i for i in discourse_type]\n",
    "# b_discourse_type = ['B-'+i for i in discourse_type]\n",
    "# labels_to_ids = {k:v for v,k in enumerate(['O']+i_discourse_type+b_discourse_type)}\n",
    "# ids_to_labels = {k:v for v,k in labels_to_ids.items()}\n",
    "\n",
    "# segment_param = {\n",
    "# \"Lead\":                 {'min_proba':[0.47,0.41],'begin_proba':1.00,'min_sep':40,'min_length': 5},\n",
    "# \"Position\":             {'min_proba':[0.45,0.40],'begin_proba':0.90,'min_sep':21,'min_length': 3},\n",
    "# \"Evidence\":             {'min_proba':[0.50,0.40],'begin_proba':0.56,'min_sep': 2,'min_length':21},\n",
    "# \"Claim\":                {'min_proba':[0.40,0.30],'begin_proba':0.30,'min_sep':10,'min_length': 1},\n",
    "# \"Concluding Statement\": {'min_proba':[0.58,0.25],'begin_proba':0.93,'min_sep':50,'min_length': 5},\n",
    "# \"Counterclaim\":         {'min_proba':[0.45,0.25],'begin_proba':0.70,'min_sep':35,'min_length': 6},\n",
    "# \"Rebuttal\":             {'min_proba':[0.37,0.34],'begin_proba':0.70,'min_sep':45,'min_length': 5},\n",
    "# }\n",
    "\n",
    "# test_predictionstring = after_deal(test_feat, labels_to_ids, segment_param,print)\n",
    "# del test_feat\n",
    "# gc.collect()\n",
    "\n",
    "# print(test_predictionstring.head())\n",
    "# test_predictionstring.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882e37d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:11.575562Z",
     "iopub.status.busy": "2022-03-15T13:48:11.574625Z",
     "iopub.status.idle": "2022-03-15T13:48:11.585043Z",
     "shell.execute_reply": "2022-03-15T13:48:11.585927Z",
     "shell.execute_reply.started": "2022-03-13T05:31:17.469133Z"
    },
    "papermill": {
     "duration": 0.068282,
     "end_time": "2022-03-15T13:48:11.586152",
     "exception": false,
     "start_time": "2022-03-15T13:48:11.517870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_pred = pickle.load(open('./ronghe.pkl','+rb'))\n",
    "data_pred['pred'] = data_pred['pred'].apply(lambda x: x/x.sum(axis=1).reshape(-1,1))  # 重要不要遗漏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a68706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:11.663215Z",
     "iopub.status.busy": "2022-03-15T13:48:11.662402Z",
     "iopub.status.idle": "2022-03-15T13:48:12.222099Z",
     "shell.execute_reply": "2022-03-15T13:48:12.221620Z",
     "shell.execute_reply.started": "2022-03-13T05:31:17.487048Z"
    },
    "papermill": {
     "duration": 0.59532,
     "end_time": "2022-03-15T13:48:12.222232",
     "exception": false,
     "start_time": "2022-03-15T13:48:11.626912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "from numba import jit\n",
    "data_path = '../input/feedback-prize-2021/'\n",
    "\n",
    "files = os.listdir(data_path+'test')\n",
    "IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "\n",
    "t = {}\n",
    "for f in IDS:\n",
    "    t[f.replace('.txt','')] = len(open(f'{data_path}test/{f}.txt', 'r').read().split())\n",
    "IDS = sorted(t.keys(), key=lambda x: t[x])\n",
    "\n",
    "id2label = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
    "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "class CONFIG:\n",
    "    def __init__(self):\n",
    "        self.max_length = 4096\n",
    "        \n",
    "config = CONFIG() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3ea5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:12.301506Z",
     "iopub.status.busy": "2022-03-15T13:48:12.300423Z",
     "iopub.status.idle": "2022-03-15T13:48:12.305921Z",
     "shell.execute_reply": "2022-03-15T13:48:12.305481Z",
     "shell.execute_reply.started": "2022-03-13T05:31:18.145285Z"
    },
    "papermill": {
     "duration": 0.05175,
     "end_time": "2022-03-15T13:48:12.306051",
     "exception": false,
     "start_time": "2022-03-15T13:48:12.254301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic_off_map = data_pred[['id','offset_mapping']].set_index('id')['offset_mapping'].to_dict()\n",
    "dic_txt = data_pred[['id','text']].set_index('id')['text'].to_dict()\n",
    "\n",
    "def change_wenfeng(x):\n",
    "    res1  = x[:,8:].sum(axis=1)\n",
    "    res2 = np.zeros((len(res1), 8))\n",
    "    \n",
    "    label_map = {0:5, 1:3, 2:2, 3:1, 4:4, 5:6, 6:7, 7:0}\n",
    "    for i in range(8):\n",
    "        if i == 7:\n",
    "            res2[:,i] = x[:,label_map[i]]\n",
    "        else:\n",
    "            res2[:,i] = x[:,[label_map[i], label_map[i]+7]].sum(axis=1)\n",
    "\n",
    "    return res1, res2\n",
    "\n",
    "preds1_mean = {}\n",
    "preds2_mean = {}\n",
    "for irow,row in data_pred.iterrows():\n",
    "    t1, t2 = change_wenfeng(row.pred)\n",
    "    preds1_mean[row.id] = t1.astype('float64')\n",
    "    preds2_mean[row.id] = t2.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecdcdce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:12.393955Z",
     "iopub.status.busy": "2022-03-15T13:48:12.378229Z",
     "iopub.status.idle": "2022-03-15T13:48:15.444130Z",
     "shell.execute_reply": "2022-03-15T13:48:15.444579Z",
     "shell.execute_reply.started": "2022-03-13T05:31:18.171353Z"
    },
    "papermill": {
     "duration": 3.10658,
     "end_time": "2022-03-15T13:48:15.444740",
     "exception": false,
     "start_time": "2022-03-15T13:48:12.338160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator PCA from version 1.0.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(8, 16)\n",
       "  (lstm): LSTM(1, 32, bidirectional=True)\n",
       "  (fc0): Linear(in_features=80, out_features=80, bias=True)\n",
       "  (fc1): Linear(in_features=80, out_features=1, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=8, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(8,16)\n",
    "        self.lstm = nn.LSTM(1, 32, num_layers=1, bidirectional=True)\n",
    "        self.fc0 = nn.Linear(64+16, 64+16)\n",
    "        self.fc1 = nn.Linear(64+16, 1)\n",
    "        self.fc2 = nn.Linear(64+16, 8)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.init_parameters()\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        x = x.unsqueeze(2)\n",
    "        sequence_output_l = (x.permute(1, 0, 2))\n",
    "        sequence_output_l, _ = self.lstm(sequence_output_l)\n",
    "#         print(sequence_output_l.shape)\n",
    "        sequence_output_l = sequence_output_l.permute(1, 0, 2)[:,-1,:]\n",
    "        sequence_output_l = torch.cat([sequence_output_l,self.embedding(x2).squeeze(1) * 0],1)\n",
    "        sequence_output_l = self.dropout(sequence_output_l)\n",
    "        sequence_output_l = nn.ReLU()(self.fc0(sequence_output_l))\n",
    "        output1 = nn.Sigmoid()(self.fc1(sequence_output_l))\n",
    "        output2 = nn.Sigmoid()(self.fc2(sequence_output_l))\n",
    "        return output1,output2\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "def transform_score(x,n = 32):\n",
    "    res = np.zeros(n+1)+0.01\n",
    "    res_count = np.zeros(n+1)+0.01\n",
    "    for i in range(len(x)):\n",
    "        start_index = int(np.floor((i * n)/len(x)))\n",
    "        end_index = int(np.ceil(((i + 1) * n)/len(x)))\n",
    "        for index in range(start_index,end_index+1):\n",
    "            res[index] += x[i]\n",
    "            res_count[index] += 1\n",
    "    res = res/res_count\n",
    "    return res\n",
    "\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "import pickle\n",
    "pca = pickle.load(open('../input/fdddw3/pcamodel.h5', 'rb'))\n",
    "lstmmodels = torch.load('../input/fdddw3/lstmmodel1.h5').cuda()\n",
    "lstmmodels.eval()\n",
    "lstmmodels2 = torch.load('../input/fdddw3/lstmmodel4.h5').cuda()\n",
    "lstmmodels2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa2e0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:15.556873Z",
     "iopub.status.busy": "2022-03-15T13:48:15.547246Z",
     "iopub.status.idle": "2022-03-15T13:48:15.782174Z",
     "shell.execute_reply": "2022-03-15T13:48:15.782678Z",
     "shell.execute_reply.started": "2022-03-13T05:31:22.180139Z"
    },
    "papermill": {
     "duration": 0.305075,
     "end_time": "2022-03-15T13:48:15.782840",
     "exception": false,
     "start_time": "2022-03-15T13:48:15.477765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# recall_thre = { \n",
    "#     \"Lead\": 0.07,\n",
    "#     \"Position\": 0.07,\n",
    "#     \"Evidence\": 0.07,\n",
    "#     \"Claim\": 0.06,\n",
    "#     \"Concluding Statement\": 0.07,\n",
    "#     \"Counterclaim\": 0.04,\n",
    "#     \"Rebuttal\": 0.03,\n",
    "# }\n",
    "recall_thre = { \n",
    "    \"Lead\": 0.07,\n",
    "    \"Position\": 0.06,\n",
    "    \"Evidence\": 0.07,\n",
    "    \"Claim\": 0.06,\n",
    "    \"Concluding Statement\": 0.07,\n",
    "    \"Counterclaim\": 0.03,\n",
    "    \"Rebuttal\": 0.02,\n",
    "}\n",
    "\n",
    "L_k = {\n",
    "    \"Evidence\": 0.85,\n",
    "    \"Rebuttal\": 0.6,\n",
    "}\n",
    "\n",
    "def deal_predictionstring(df):\n",
    "    new_predictionstring = []\n",
    "    new_pos_list = []\n",
    "    flag_list = []\n",
    "    thre = 0.8\n",
    "    for id, typ, pos, (start, end) in df.values:\n",
    "        flag = 0\n",
    "        L = round(max(1, (pos[1]-pos[0]+1)*0.25))\n",
    "\n",
    "        pos_left = max(0, pos[0]-L)\n",
    "        pos_right = min(len(preds1_mean[id]), pos[1]+1+L)\n",
    "        \n",
    "        if start<10:\n",
    "            left_thre = 2\n",
    "        else:\n",
    "            left_thre = max(preds1_mean[id][pos[0]], 1-preds2_mean[id][pos_left:pos[0],label2id[typ]].min())\n",
    "        \n",
    "        if pos[1] >= len(preds1_mean[id])-10:\n",
    "            right_thre=2\n",
    "        else:\n",
    "            right_thre = max(preds1_mean[id][pos[1]+1:pos_right].max(), 1-preds2_mean[id][pos[1]+1:pos_right, label2id[typ]].min())\n",
    "        \n",
    "        if left_thre>thre and right_thre>thre:\n",
    "\n",
    "            L = math.ceil((pos[1]-pos[0]+1)*L_k.get(typ, 0.65))\n",
    "\n",
    "            tmp = {}\n",
    "            for i in range(pos[0], pos[1]):\n",
    "                if i+L>pos[1]:\n",
    "                    break\n",
    "                tmp[i] = np.sum(preds2_mean[id][i:i+L+1,label2id[typ]])\n",
    "            if len(tmp)==0:\n",
    "                new_pos = pos\n",
    "            else:\n",
    "                flag = min(left_thre, right_thre)\n",
    "                new_start = max(tmp.keys(), key=lambda x:tmp[x])\n",
    "                new_pos = (new_start,new_start+L)\n",
    "\n",
    "        else:\n",
    "            new_pos = pos\n",
    "\n",
    "        off_map = dic_off_map[id]\n",
    "        txt = dic_txt[id]\n",
    "        txt_max = len(txt.split())\n",
    "\n",
    "        start_word = len(txt[:off_map[new_pos[0]][0]].split())\n",
    "\n",
    "        L = len(txt[off_map[new_pos[0]][0]:off_map[new_pos[1]][1]].split())\n",
    "        end_word = min(txt_max, start_word+L) - 1\n",
    "\n",
    "        new_predictionstring.append([start_word, end_word])\n",
    "        new_pos_list.append(new_pos)\n",
    "        flag_list.append(flag)\n",
    "        \n",
    "    df_new = df.copy()\n",
    "    df_new['pos'] = new_pos_list\n",
    "    df_new['predictionstring'] = new_predictionstring\n",
    "    df_new['flag'] = flag_list\n",
    "    \n",
    "    df_new = pd.concat([df_new, df.loc[df_new[(df_new.flag>=0.8) & (df_new.flag<0.95)].index]])\n",
    "    df_new = df_new.reset_index(drop=True)\n",
    "    df_new['flag'].fillna(0,inplace=True)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def get_recall(id):\n",
    "    all_predictions = []\n",
    "\n",
    "    pred1_np = np.array(preds1_mean[id])\n",
    "    pred2_np_all = np.array(preds2_mean[id])\n",
    "\n",
    "    off_map = dic_off_map[id]\n",
    "    off_map_len = len(off_map) if off_map[-1][1] != 0 else len(off_map)-1\n",
    "    max_length = min(config.max_length, off_map_len)\n",
    "    for class_num in range(7):\n",
    "        thre = recall_thre[id2label[class_num]]\n",
    "        pred2_np = pred2_np_all[:, class_num]\n",
    "\n",
    "        i_start = 0\n",
    "        while i_start < max_length:\n",
    "            i = 0\n",
    "            if pred1_np[i_start] > thre and pred2_np[i_start:i_start+10].max() > thre: #开头\n",
    "                i = i_start + 1\n",
    "                if i>=max_length: break\n",
    "                while pred1_np[i] < (1-thre) and pred2_np[i:i+10].max() > thre: # 结束\n",
    "                    cond = any([\n",
    "                        i+1==max_length,\n",
    "                        pred1_np[i] > thre,\n",
    "                        i+1<max_length and pred2_np[i] < 0.6 and pred2_np[i] - pred2_np[i+1] > thre\n",
    "                    ])\n",
    "                    if i>i_start+1 and cond:\n",
    "                        all_predictions.append((id, id2label[class_num], [i_start, i]))\n",
    "                    i += 1\n",
    "                    if i>=max_length: break\n",
    "\n",
    "            if i != 0:\n",
    "                if i == max_length:\n",
    "                    i -=1\n",
    "\n",
    "                all_predictions.append((id, id2label[class_num], [i_start, i]))\n",
    "            i_start += 1\n",
    "                \n",
    "    df_recall = pd.DataFrame(all_predictions, columns=['id', 'class', 'pos'])\n",
    "    \n",
    "    predictionstring = []\n",
    "    for cache in df_recall.values:\n",
    "        id = cache[0]\n",
    "        pos = cache[2]\n",
    "        off_map = dic_off_map[id]\n",
    "        txt = dic_txt[id]\n",
    "        txt_max = len(txt.split())\n",
    "\n",
    "        start_word = len(txt[:off_map[pos[0]][0]].split())\n",
    "\n",
    "        L = len(txt[off_map[pos[0]][0]:off_map[pos[1]][1]].split())\n",
    "        end_word = min(txt_max, start_word+L) - 1\n",
    "\n",
    "        predictionstring.append([start_word, end_word])\n",
    "\n",
    "    df_recall['predictionstring'] = predictionstring\n",
    "\n",
    "    return deal_predictionstring(df_recall)\n",
    "#     return df_recall\n",
    "\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def feat_speedup(arr):\n",
    "    r_max, r_min, r_sum = -1e5,1e5,0\n",
    "    for x in arr:\n",
    "        r_max = max(r_max, x)\n",
    "        r_min = min(r_min, x)\n",
    "        r_sum += x\n",
    "    return r_max, r_min, r_sum, r_sum/len(arr)\n",
    "\n",
    "np_lin = np.linspace(0,1,7)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def sorted_quantile(array, q):\n",
    "    n = len(array)\n",
    "    index = (n - 1) * q\n",
    "    left = int(index)\n",
    "    fraction = index - left\n",
    "    right = left\n",
    "    right = right + int(fraction > 0)\n",
    "    i, j = array[left], array[right]\n",
    "    return i + (j - i) * fraction\n",
    "\n",
    "def get_percentile(array):\n",
    "    x = np.sort(array)\n",
    "    n = len(x)-1\n",
    "    return x[[int(n*t) for t in np_lin[1:-1]]]\n",
    "\n",
    "\n",
    "def tuple_map(offset_mapping,threshold):\n",
    "    # 无意义0 其他从1 2 3排序\n",
    "    paragraph_rk = []\n",
    "    rk = 0\n",
    "    last = 1\n",
    "    for token_index in offset_mapping:\n",
    "        if len(threshold) == 0:\n",
    "            paragraph_rk.append(1)   # 只有一段\n",
    "        elif token_index[1] <= threshold[rk][1]:\n",
    "            last = max(rk+1,last)\n",
    "            paragraph_rk.append(last)  # 左区间\n",
    "        else: \n",
    "            last = max(rk+2,last)\n",
    "            paragraph_rk.append(last)  # 右区间\n",
    "            if rk + 1 < len(threshold) - 1: #判断加一是否溢出\n",
    "                rk += 1\n",
    "            \n",
    "    return paragraph_rk\n",
    "\n",
    "\n",
    "def get_pos_feat(text, offset_mapping):\n",
    "    # 总共几段\n",
    "    paragraph_cnt = len(text.split('\\n\\n')) + 1\n",
    "    # 第几段\n",
    "    paragraph_th = [m.span() for m in re.finditer('\\n\\n',text)]\n",
    "    paragraph_rk = tuple_map(offset_mapping,paragraph_th)\n",
    "    # 倒数第几段\n",
    "    paragraph_rk_r = [paragraph_cnt-rk+1 if rk!=0 else 0 for rk in paragraph_rk]\n",
    "    # 总共几句\n",
    "    sentence_th = []\n",
    "    for i,v in enumerate([m.span() for m in re.finditer('\\n\\n|\\.|,|\\?|\\!',text)]):\n",
    "        if i == 0:\n",
    "            sentence_th.append(list(v))\n",
    "        else:\n",
    "            if v[0]==sentence_th[-1][-1]:\n",
    "                sentence_th[-1][-1] = v[-1]\n",
    "            else:\n",
    "                sentence_th.append(list(v))\n",
    "    sentence_cnt = len(sentence_th) + 1\n",
    "    # 第几句\n",
    "    sentence_rk = tuple_map(offset_mapping,sentence_th)\n",
    "    # 导数第几句\n",
    "    sentence_rk_r = [sentence_cnt-rk+1 if rk!=0 else 0 for rk in sentence_rk]\n",
    "\n",
    "    # 属于段落的第几句\n",
    "    last_garagraph_cnt = 0\n",
    "    sentence_rk_of_paragraph = []\n",
    "    for i in range(len(offset_mapping)):\n",
    "        sentence_rk_of_paragraph.append(sentence_rk[i]-last_garagraph_cnt)\n",
    "        if i+1 == len(offset_mapping) or paragraph_rk[i]!=paragraph_rk[i+1]:\n",
    "            last_garagraph_cnt = sentence_rk[i]\n",
    "\n",
    "    # 当前段落几句\n",
    "    sentence_cnt_of_paragraph = []\n",
    "    last_max = None\n",
    "    for i in range(1,len(offset_mapping)+1):\n",
    "        if i==1 or paragraph_rk[-i] != paragraph_rk[-i+1]:\n",
    "            last_max = sentence_rk_of_paragraph[-i]\n",
    "        sentence_cnt_of_paragraph.append(last_max)\n",
    "    sentence_cnt_of_paragraph = sentence_cnt_of_paragraph[::-1]\n",
    "    # 属于段落的倒数第几句\n",
    "    sentence_rk_r_of_paragraph = [s_cnt-rk+1 if rk!=0 else 0 for s_cnt,rk in zip(sentence_cnt_of_paragraph,sentence_rk_of_paragraph)]\n",
    "\n",
    "    return paragraph_cnt,sentence_cnt,paragraph_rk,paragraph_rk_r,sentence_rk,sentence_rk_r, \\\n",
    "            sentence_cnt_of_paragraph,sentence_rk_of_paragraph,sentence_rk_r_of_paragraph\n",
    "\n",
    "\n",
    "lgb_columns = pickle.load(open('../input/feedback-lgb7471/lgb_columns.pkl','rb'))\n",
    "lgb_columns = lgb_columns[:14] + ['pca_f'+str(i) for i in range(8)]  + ['pca2_f'+str(i) for i in range(8)] + lgb_columns[14:]\n",
    "\n",
    "def fun_get_feat(id):\n",
    "    df_feat = []\n",
    "    df_feat2 = []\n",
    "    data_sub = get_recall(id)\n",
    "    txt = dic_txt[id]\n",
    "    off_map = dic_off_map[id]\n",
    "    txt_feat = get_pos_feat(txt, off_map)\n",
    "   \n",
    "    preds1_all = preds1_mean[id]\n",
    "    preds_type = preds2_mean[id].argmax(axis=-1)\n",
    "    \n",
    "    text_char_length = len(txt)\n",
    "    text_word_length = len(txt.split())\n",
    "    text_token_length = len(off_map)\n",
    "    repeat_key_set = set()\n",
    "    for cache in data_sub.values:\n",
    "        id = cache[0]\n",
    "        typ = cache[1]\n",
    "        start, end = cache[2]\n",
    "        prediction = cache[3]\n",
    "        repeat_key = id+str(typ)+str(start)+str(end)\n",
    "        if repeat_key in repeat_key_set:\n",
    "            continue\n",
    "        repeat_key_set.add(repeat_key)\n",
    "        dic = {k:np.nan for k in lgb_columns}\n",
    "#         dic={'id': id}\n",
    "        dic['id'] = id\n",
    "        dic['pos'] = cache[2]\n",
    "        dic['class'] = label2id[typ]\n",
    "        dic['post_flag'] = cache[4]\n",
    "\n",
    "        # 段落特征\n",
    "#         txt_feat  = dic_txt_feat[id]\n",
    "        dic['paragraph_cnt'] = txt_feat[0]\n",
    "        dic['sentence_cnt'] = txt_feat[1]\n",
    "        dic['paragraph_rk'] = txt_feat[2][start]\n",
    "        dic['paragraph_rk_r'] = txt_feat[3][end]\n",
    "        dic['sentence_rk'] = txt_feat[4][start]\n",
    "        dic['sentence_rk_r'] = txt_feat[5][end]\n",
    "        dic['sentence_cnt_of_paragraph'] = txt_feat[6][start]\n",
    "        dic['sentence_cnt_of_paragraph2'] = txt_feat[6][end]\n",
    "        dic['sentence_rk_of_paragraph'] = txt_feat[7][start]\n",
    "        dic['sentence_rk_r_of_paragraph'] = txt_feat[8][end]\n",
    "        dic['sub_paragraph_cnt'] = txt_feat[2][end] - txt_feat[2][start]\n",
    "        dic['sub_sentence_cnt'] = txt_feat[4][end] - txt_feat[4][start]\n",
    "\n",
    "        # 段内统计特征\n",
    "        other_type = [t for t in range(8) if t != dic['class']]\n",
    "        preds2_all = preds2_mean[id][:, label2id[typ]]\n",
    "        preds4_all = preds2_mean[id][:, other_type].max(axis=-1)\n",
    "        preds1 = preds1_all[start:end+1]\n",
    "        preds2 = preds2_all[start:end+1]\n",
    "        preds4 = preds4_all[start:end+1]\n",
    "        preds2lstm = preds2_all[max(0,start - 2):min(preds2_all.shape[0] - 1,end+3)]  \n",
    "        # preds1lstm = preds1_all[max(0,start - 2):min(preds2_all.shape[0] - 1,end+3)]  \n",
    "        # preds2lstm = np.concatenate([preds2lstm,preds1lstm],1)\n",
    "        # model = lstmmodels[idlist_lstm_dict[id] % 5]\n",
    "        # output1,output2 = model(torch.tensor([([-1] * 128 + preds2lstm.reshape(-1).tolist())[-128:]]).cuda(),\n",
    "        # torch.tensor([label2id[typ]]).long().cuda())\n",
    "        # output1 = output1.cpu().detach().numpy()[0].tolist()\n",
    "        # output2 = output2.cpu().detach().numpy()[0].tolist()\n",
    "        # dic[f'lstm_f0'] = output1[0]\n",
    "        # for i in range(8):\n",
    "        #     dic[f'lstm_f{i}'] = output2[i]\n",
    "        # dic2={'id': id, 'x':[preds2lstm.reshape(-1),label2id[typ]]}\n",
    "        df_feat2.append([id,preds2lstm.reshape(-1),label2id[typ]])\n",
    "\n",
    "        word_length = prediction[-1] - prediction[0] + 1\n",
    "        \n",
    "        pca_res = pca.transform([transform_score(preds2lstm)])[0]\n",
    "        for i in range(8):\n",
    "            dic['pca_f'+str(i)] = pca_res[i] \n",
    "        pca_res = pca.transform([transform_score(preds1_all[max(0,start - 2):min(preds2_all.shape[0] - 1,end+3)])])[0]\n",
    "        for i in range(8):\n",
    "            dic['pca2_f'+str(i)] = pca_res[i] \n",
    "        dic['L1'] = word_length\n",
    "        dic['L2'] = end - start + 1\n",
    "        dic['text_char_length'] = text_char_length\n",
    "        dic['text_word_length'] = text_word_length\n",
    "        dic['text_token_length'] = text_token_length\n",
    "\n",
    "        dic['word_start'] = prediction[0]\n",
    "        dic['word_end'] = prediction[-1]\n",
    "        dic['token_start'] = start\n",
    "        dic['token_start2'] = start / text_token_length\n",
    "        dic['token_end'] = end\n",
    "        dic['token_end2'] = text_token_length - end\n",
    "        dic['token_end3'] = end / text_token_length\n",
    "        \n",
    "        dic[f'head_preds1'] = preds1[0]\n",
    "        dic[f'head2_preds1'] = preds1_all[start-1:start+2].sum()\n",
    "        if len(preds1) > 1:\n",
    "            dic[f'tail_preds1'] = preds1[-1]\n",
    "            dic['max_preds1'], dic['min_preds1'], dic['sum_preds1'], dic['mean_preds1'] = feat_speedup(preds1[1:])\n",
    "      \n",
    "        sort_idx = preds1[1:].argsort()[::-1]\n",
    "        tmp = []\n",
    "        for i in range(5):\n",
    "            if i < len(sort_idx):\n",
    "                dic[f'other_preds1_{i}'] = preds1[1+sort_idx[i]]\n",
    "                dic[f'other_preds1_idx_{i}'] = (1+sort_idx[i])/len(preds1)\n",
    "                tmp.append(preds1[1+sort_idx[i]])\n",
    "        if len(tmp):\n",
    "            dic[f'other_preds1_mean'] = np.mean(tmp)\n",
    "\n",
    "        dic[f'head_preds2'] = preds2[0]\n",
    "        dic[f'tail_preds2'] = preds2[-1]\n",
    "        dic['max_preds2'], dic['min_preds2'], dic['sum_preds2'], dic['mean_preds2'] = feat_speedup(preds2)\n",
    "\n",
    "        dic[f'head_preds4'] = preds4[0]\n",
    "        dic[f'tail_preds4'] = preds4[-1]\n",
    "        dic['max_preds4'], dic['min_preds4'], dic['sum_preds4'], dic['mean_preds4'] = feat_speedup(preds4)\n",
    "        \n",
    "        sort_idx = preds2.argsort()\n",
    "        tmp = []\n",
    "        for i in range(5):\n",
    "            if i < len(sort_idx):\n",
    "                dic[f'other_preds2_{i}'] = preds2[sort_idx[i]]\n",
    "                dic[f'other_preds2_idx_{i}'] = (sort_idx[i])/len(preds2)\n",
    "                tmp.append(preds2[sort_idx[i]])\n",
    "        if len(tmp):\n",
    "            dic[f'other_preds2_mean'] = np.mean(tmp)\n",
    "            \n",
    "            \n",
    "        for i,ntile in enumerate([sorted_quantile(preds2,i) for i in np_lin]):\n",
    "            dic[f'preds2_trend{i}'] = ntile        # 趋势分布\n",
    "        for i,ntile in enumerate(get_percentile(preds2)):\n",
    "            dic[f'preds2_ntile{i}'] = ntile        # 分位数\n",
    "        for i,ntile in enumerate([sorted_quantile(preds4,i) for i in np_lin]):\n",
    "            dic[f'preds4_trend{i}'] = ntile        # 趋势分布\n",
    "        for i,ntile in enumerate(get_percentile(preds4)):\n",
    "            dic[f'preds4_ntile{i}'] = ntile        # 分位数\n",
    "            \n",
    "            \n",
    "        for i in range(1,4):\n",
    "            if start-i >= 0:\n",
    "                dic[f'before_head2_prob{i}'] = preds2_all[start-i]\n",
    "                dic[f'before_other_prob{i}'] = preds4_all[start-i]\n",
    "                dic[f'before_other_type{i}'] = preds_type[start-i]\n",
    "                \n",
    "            if end+i < len(preds1_all):\n",
    "                dic[f'after_head2_prob{i}'] = preds2_all[end+i]\n",
    "                dic[f'after_other_prob{i}'] = preds4_all[end+i]\n",
    "                dic[f'after_other_type{i}'] = preds_type[end+i]\n",
    "\n",
    "\n",
    "        for mode in ['before', 'after']:\n",
    "            for iw, extend_L in enumerate([math.ceil(word_length/2), word_length]):\n",
    "                if mode == 'before':\n",
    "                    if start-extend_L<0:\n",
    "                        continue\n",
    "                    preds1_extend = preds1_all[start-extend_L:start]\n",
    "                    preds2_extend = preds2_all[start-extend_L:start]\n",
    "#                     preds4_extend = preds4_all[start-extend_L:start]\n",
    "                else:\n",
    "                    if end+extend_L >=len(preds1_all):\n",
    "                        continue\n",
    "                    preds1_extend = preds1_all[end+1:end+extend_L]\n",
    "                    preds2_extend = preds2_all[end+1:end+extend_L]\n",
    "#                     preds4_extend = preds4_all[end+1:end+extend_L]\n",
    "                    \n",
    "                if len(preds1_extend) == 0:\n",
    "                    continue\n",
    "                dic[f'{mode}{iw}_head_preds1'] = preds1_extend[0]\n",
    "                dic[f'{mode}{iw}_max_preds1'], dic[f'{mode}{iw}_min_preds1'], \\\n",
    "                dic[f'{mode}{iw}_sum_preds1'], dic[f'{mode}{iw}_mean_preds1'] = feat_speedup(preds1_extend)\n",
    "\n",
    "                dic[f'{mode}{iw}_head_preds2'] = preds2_extend[0]\n",
    "                dic[f'{mode}{iw}_max_preds2'], dic[f'{mode}{iw}_min_preds2'], \\\n",
    "                dic[f'{mode}{iw}_sum_preds2'], dic[f'{mode}{iw}_mean_preds2'] = feat_speedup(preds2_extend)\n",
    "\n",
    "                dic[f'{mode}{iw}_sum_preds1_rate'] = dic[f'{mode}{iw}_sum_preds1'] / dic[f'sum_preds1']\n",
    "                dic[f'{mode}{iw}_sum_preds2_rate'] = dic[f'{mode}{iw}_sum_preds2'] / dic[f'sum_preds2']\n",
    "                dic[f'{mode}{iw}_max_preds1_rate'] = dic[f'{mode}{iw}_max_preds1'] / dic[f'max_preds1']\n",
    "                dic[f'{mode}{iw}_max_preds2_rate'] = dic[f'{mode}{iw}_max_preds2'] / dic[f'max_preds2']\n",
    "\n",
    "        df_feat.append(dic)\n",
    "\n",
    "    return [df_feat,df_feat2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06de10c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:15.888455Z",
     "iopub.status.busy": "2022-03-15T13:48:15.872693Z",
     "iopub.status.idle": "2022-03-15T13:48:24.629128Z",
     "shell.execute_reply": "2022-03-15T13:48:24.628506Z",
     "shell.execute_reply.started": "2022-03-13T05:31:22.643005Z"
    },
    "papermill": {
     "duration": 8.813549,
     "end_time": "2022-03-15T13:48:24.629255",
     "exception": false,
     "start_time": "2022-03-15T13:48:15.815706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:98: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:98: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:98: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:98: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:98: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "proba_thresh = { \n",
    "    \"Lead\": 0.48,\n",
    "    \"Position\": 0.4,\n",
    "    \"Evidence\": 0.44,\n",
    "    \"Claim\": 0.39,\n",
    "    \"Concluding Statement\": 0.51,\n",
    "    \"Counterclaim\": 0.34,\n",
    "    \"Rebuttal\": 0.3,\n",
    "}\n",
    "\n",
    "inter_thresh = { \n",
    "    \"Lead\": 0.09,\n",
    "    \"Position\": 0.19,\n",
    "    \"Evidence\": 0.07,\n",
    "    \"Claim\": 0.27,\n",
    "    \"Concluding Statement\": 0.15,\n",
    "    \"Counterclaim\": 0.25,\n",
    "    \"Rebuttal\": 0.21,\n",
    "}\n",
    "\n",
    "def post_choice(df):\n",
    "    rtn = []\n",
    "    for k,group in df.groupby(['id','class']):\n",
    "        group = group.sort_values('lgb_prob',ascending=False)\n",
    "\n",
    "        preds_range = []\n",
    "        for irow, row in group.iterrows():\n",
    "            start = row.word_start\n",
    "            end = row.word_end\n",
    "            L1 = end-start+1\n",
    "            flag = 0\n",
    "            for pos_range in preds_range:\n",
    "                L2 = pos_range[1] - pos_range[0] + 1\n",
    "                intersection = (min(end, pos_range[1]) - max(start, pos_range[0]) + 1) / L1\n",
    "                inter_t = inter_thresh[row['class']]\n",
    "                if intersection>inter_t and (inter_t<=L1/L2<=1 or inter_t<=L2/L1<=1):\n",
    "                    flag = 1\n",
    "                    break\n",
    "\n",
    "            if flag == 0:\n",
    "                preds_range.append((start, end, row.lgb_prob))\n",
    "                rtn.append((row.id, row['class'], row.pos, row.word_start, row.word_end, row.lgb_prob))\n",
    "    rtn = pd.DataFrame(rtn, columns=['id','class','pos','start','end','lgb_prob'])\n",
    "    return rtn\n",
    "\n",
    "\n",
    "data_splits = np.array_split(IDS, round(len(IDS)/2))\n",
    "\n",
    "lgb_model = pickle.load(open(f'../input/feedback-lgb7471/lgb_fold{0}.pkl','rb'))\n",
    "lgb_model1 = pickle.load(open(f'../input/fdddw3/lgb_fold2.pkl','rb'))\n",
    "lgb_model2 = pickle.load(open(f'../input/fdddw3/lgb_fold4.pkl','rb'))\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "# for id_sub in data_splits:\n",
    "#     results = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n",
    "#                 delayed(fun_get_feat)(id) for id in id_sub)\n",
    "#     df_feat = []\n",
    "#     df_feat2 = []\n",
    "#     for res in results:\n",
    "#         df_feat += res[0]\n",
    "#         df_feat2 += res[1]\n",
    "for id in (IDS):\n",
    "#     for ii in tqdm(range(1000)):\n",
    "    df_feat,df_feat2 = fun_get_feat(id)\n",
    "\n",
    "    df_feat = pd.DataFrame(df_feat)\n",
    "\n",
    "    X = []\n",
    "    X2 = []\n",
    "    for i in range(len(df_feat2)):\n",
    "        X.append(([-1] * 128 + df_feat2[i][1].reshape(-1).tolist())[-128:])\n",
    "        X2.append([df_feat2[i][2]])\n",
    "\n",
    "    model = lstmmodels\n",
    "    model2 = lstmmodels2\n",
    "    pos = 0\n",
    "    df_feat3 = np.zeros((len(df_feat),9))\n",
    "    while(True):\n",
    "        output1,output2 = model(torch.tensor(X[pos:pos + 32]).cuda(),torch.tensor(X2[pos:pos + 32]).cuda().long())\n",
    "\n",
    "        df_feat3[pos:pos + output1.shape[0],:1] += output1.cpu().detach().numpy() * 0.5\n",
    "        df_feat3[pos:pos + output1.shape[0],1:] += output2.cpu().detach().numpy() * 0.5\n",
    "        output1,output2 = model2(torch.tensor(X[pos:pos + 32]).cuda(),torch.tensor(X2[pos:pos + 32]).cuda().long())\n",
    "        df_feat3[pos:pos + output1.shape[0],:1] += output1.cpu().detach().numpy() * 0.5\n",
    "        df_feat3[pos:pos + output1.shape[0],1:] += output2.cpu().detach().numpy() * 0.5\n",
    "        \n",
    "        pos += 32\n",
    "        if pos >= len(X):\n",
    "            break\n",
    "\n",
    "\n",
    "    df_feat3 = pd.DataFrame(df_feat3)\n",
    "    df_feat3.columns = ['lstmf' + str(i) for i in range(df_feat3.shape[1])]\n",
    "    df_feat = pd.concat([df_feat,df_feat3.iloc[:,:]],1)\n",
    "#     lgb_preds = lgb_model1.predict(df_feat.drop(['id','pos'],axis=1))\n",
    "    lgb_preds = (lgb_model1.predict(df_feat.drop(['id','pos'],axis=1)) + lgb_model2.predict(df_feat.drop(['id','pos'],axis=1)))/2\n",
    "#     print(lgb_model1.predict(df_feat.drop(['id','pos'],axis=1)))\n",
    "#     print(lgb_model2.predict(df_feat.drop(['id','pos'],axis=1)))\n",
    "    df_final = df_feat[['id', 'class','pos', 'word_start','word_end']].copy()\n",
    "    df_final['lgb_prob'] = lgb_preds\n",
    "    df_final['class'] = df_final['class'].map(lambda x:id2label[x])\n",
    "\n",
    "    df_final['thre'] = df_final['class'].map(lambda x: proba_thresh[x])\n",
    "    df_final = df_final[df_final.lgb_prob>=df_final.thre]\n",
    "    df_final = post_choice(df_final)\n",
    "\n",
    "    sub = pd.concat([sub, df_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb7fa10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:24.706399Z",
     "iopub.status.busy": "2022-03-15T13:48:24.704715Z",
     "iopub.status.idle": "2022-03-15T13:48:24.706958Z",
     "shell.execute_reply": "2022-03-15T13:48:24.707394Z",
     "shell.execute_reply.started": "2022-03-13T05:31:32.784915Z"
    },
    "papermill": {
     "duration": 0.04334,
     "end_time": "2022-03-15T13:48:24.707527",
     "exception": false,
     "start_time": "2022-03-15T13:48:24.664187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# from joblib import Parallel, delayed\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# proba_thresh = { \n",
    "#     \"Lead\": 0.45,\n",
    "#     \"Position\": 0.4,\n",
    "#     \"Evidence\": 0.45,\n",
    "#     \"Claim\": 0.35,\n",
    "#     \"Concluding Statement\": 0.5,\n",
    "#     \"Counterclaim\": 0.35,\n",
    "#     \"Rebuttal\": 0.3,\n",
    "# }\n",
    "\n",
    "# inter_thresh = { \n",
    "#     \"Lead\": 0.15,\n",
    "#     \"Position\": 0.15,\n",
    "#     \"Evidence\": 0.15,\n",
    "#     \"Claim\": 0.25,\n",
    "#     \"Concluding Statement\": 0.15,\n",
    "#     \"Counterclaim\": 0.25,\n",
    "#     \"Rebuttal\": 0.25,\n",
    "# }\n",
    "\n",
    "\n",
    "# def post_choice(df):\n",
    "#     rtn = []\n",
    "#     for k,group in df.groupby(['id','class']):\n",
    "#         group = group.sort_values('lgb_prob',ascending=False)\n",
    "\n",
    "#         preds_range = []\n",
    "#         for irow, row in group.iterrows():\n",
    "#             start = row.word_start\n",
    "#             end = row.word_end\n",
    "#             L1 = end-start+1\n",
    "#             flag = 0\n",
    "#             for pos_range in preds_range:\n",
    "#                 L2 = pos_range[1] - pos_range[0] + 1\n",
    "#                 intersection = (min(end, pos_range[1]) - max(start, pos_range[0]) + 1) / L1\n",
    "#                 inter_t = inter_thresh[row['class']]\n",
    "#                 if intersection>inter_t and (inter_t<=L1/L2<=1 or inter_t<=L2/L1<=1):\n",
    "#                     flag = 1\n",
    "#                     break\n",
    "\n",
    "#             if flag == 0:\n",
    "#                 preds_range.append((start, end, row.lgb_prob))\n",
    "#                 rtn.append((row.id, row['class'], row.pos, row.word_start, row.word_end, row.lgb_prob))\n",
    "#     rtn = pd.DataFrame(rtn, columns=['id','class','pos','start','end','lgb_prob'])\n",
    "#     return rtn\n",
    "\n",
    "\n",
    "# data_splits = np.array_split(IDS, round(len(IDS)/2))\n",
    "\n",
    "# lgb_model = pickle.load(open(f'../input/feedback-lgb7471/lgb_fold{0}.pkl','rb'))\n",
    "\n",
    "# sub = pd.DataFrame()\n",
    "# for id_sub in data_splits:\n",
    "# # for id in tqdm(IDS):\n",
    "# #     df_feat = pd.DataFrame(fun_get_feat(id))\n",
    "#     for i in tqdm(range(1000)):\n",
    "#         results = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n",
    "#                     delayed(fun_get_feat)(id) for id in id_sub)\n",
    "#         df_feat = []\n",
    "#         for res in results:\n",
    "#             df_feat += res\n",
    "#         df_feat = pd.DataFrame(df_feat)\n",
    "\n",
    "#         lgb_preds = lgb_model.predict(df_feat.drop(['id','pos'],axis=1))\n",
    "\n",
    "#         df_final = df_feat[['id', 'class','pos', 'word_start','word_end']].copy()\n",
    "#         df_final['lgb_prob'] = lgb_preds\n",
    "#         df_final['class'] = df_final['class'].map(lambda x:id2label[x])\n",
    "\n",
    "#         df_final['thre'] = df_final['class'].map(lambda x: proba_thresh[x])\n",
    "#         df_final = df_final[df_final.lgb_prob>=df_final.thre]\n",
    "#         df_final = post_choice(df_final)\n",
    "\n",
    "#         sub = pd.concat([sub, df_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b1bef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:24.782658Z",
     "iopub.status.busy": "2022-03-15T13:48:24.782020Z",
     "iopub.status.idle": "2022-03-15T13:48:24.784952Z",
     "shell.execute_reply": "2022-03-15T13:48:24.784520Z",
     "shell.execute_reply.started": "2022-03-13T05:31:32.796009Z"
    },
    "papermill": {
     "duration": 0.043304,
     "end_time": "2022-03-15T13:48:24.785073",
     "exception": false,
     "start_time": "2022-03-15T13:48:24.741769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictionstring(df):\n",
    "    predictionstring = []\n",
    "    for cache in df.values:\n",
    "        predictionstring.append(' '.join(list(map(str, range(cache[3], cache[4]+1)))))\n",
    "    return predictionstring\n",
    "\n",
    "\n",
    "sub['predictionstring'] = get_predictionstring(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad5cebff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:24.860741Z",
     "iopub.status.busy": "2022-03-15T13:48:24.859951Z",
     "iopub.status.idle": "2022-03-15T13:48:24.867705Z",
     "shell.execute_reply": "2022-03-15T13:48:24.867169Z",
     "shell.execute_reply.started": "2022-03-13T05:31:32.810926Z"
    },
    "papermill": {
     "duration": 0.048729,
     "end_time": "2022-03-15T13:48:24.867851",
     "exception": false,
     "start_time": "2022-03-15T13:48:24.819122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[['id', 'class', 'predictionstring']].to_csv('submission.csv',index=False)\n",
    "# sub.sample(round(0.9*len(sub)))[['id', 'class', 'predictionstring']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd92316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T13:48:24.952629Z",
     "iopub.status.busy": "2022-03-15T13:48:24.951718Z",
     "iopub.status.idle": "2022-03-15T13:48:24.969500Z",
     "shell.execute_reply": "2022-03-15T13:48:24.970037Z",
     "shell.execute_reply.started": "2022-03-13T05:31:32.827589Z"
    },
    "papermill": {
     "duration": 0.062433,
     "end_time": "2022-03-15T13:48:24.970196",
     "exception": false,
     "start_time": "2022-03-15T13:48:24.907763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>pos</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>lgb_prob</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>[182, 193]</td>\n",
       "      <td>154</td>\n",
       "      <td>162</td>\n",
       "      <td>0.983745</td>\n",
       "      <td>154 155 156 157 158 159 160 161 162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>[173, 183]</td>\n",
       "      <td>146</td>\n",
       "      <td>155</td>\n",
       "      <td>0.972377</td>\n",
       "      <td>146 147 148 149 150 151 152 153 154 155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>[164, 173]</td>\n",
       "      <td>138</td>\n",
       "      <td>146</td>\n",
       "      <td>0.962529</td>\n",
       "      <td>138 139 140 141 142 143 144 145 146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>(1194, 1208)</td>\n",
       "      <td>972</td>\n",
       "      <td>986</td>\n",
       "      <td>0.658376</td>\n",
       "      <td>972 973 974 975 976 977 978 979 980 981 982 98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>(892, 909)</td>\n",
       "      <td>721</td>\n",
       "      <td>738</td>\n",
       "      <td>0.543539</td>\n",
       "      <td>721 722 723 724 725 726 727 728 729 730 731 73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Claim</td>\n",
       "      <td>[529, 549]</td>\n",
       "      <td>424</td>\n",
       "      <td>441</td>\n",
       "      <td>0.455102</td>\n",
       "      <td>424 425 426 427 428 429 430 431 432 433 434 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>(1228, 1288)</td>\n",
       "      <td>993</td>\n",
       "      <td>1044</td>\n",
       "      <td>0.994751</td>\n",
       "      <td>993 994 995 996 997 998 999 1000 1001 1002 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>[196, 524]</td>\n",
       "      <td>164</td>\n",
       "      <td>420</td>\n",
       "      <td>0.990286</td>\n",
       "      <td>164 165 166 167 168 169 170 171 172 173 174 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>[577, 910]</td>\n",
       "      <td>465</td>\n",
       "      <td>738</td>\n",
       "      <td>0.982147</td>\n",
       "      <td>465 466 467 468 469 470 471 472 473 474 475 47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>[987, 1209]</td>\n",
       "      <td>802</td>\n",
       "      <td>986</td>\n",
       "      <td>0.966533</td>\n",
       "      <td>802 803 804 805 806 807 808 809 810 811 812 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Lead</td>\n",
       "      <td>(2, 99)</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>0.966816</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>Position</td>\n",
       "      <td>(151, 163)</td>\n",
       "      <td>126</td>\n",
       "      <td>137</td>\n",
       "      <td>0.897904</td>\n",
       "      <td>126 127 128 129 130 131 132 133 134 135 136 137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                 class           pos  start   end  lgb_prob  \\\n",
       "0   18409261F5C2                 Claim    [182, 193]    154   162  0.983745   \n",
       "1   18409261F5C2                 Claim    [173, 183]    146   155  0.972377   \n",
       "2   18409261F5C2                 Claim    [164, 173]    138   146  0.962529   \n",
       "3   18409261F5C2                 Claim  (1194, 1208)    972   986  0.658376   \n",
       "4   18409261F5C2                 Claim    (892, 909)    721   738  0.543539   \n",
       "5   18409261F5C2                 Claim    [529, 549]    424   441  0.455102   \n",
       "6   18409261F5C2  Concluding Statement  (1228, 1288)    993  1044  0.994751   \n",
       "7   18409261F5C2              Evidence    [196, 524]    164   420  0.990286   \n",
       "8   18409261F5C2              Evidence    [577, 910]    465   738  0.982147   \n",
       "9   18409261F5C2              Evidence   [987, 1209]    802   986  0.966533   \n",
       "10  18409261F5C2                  Lead       (2, 99)      1    84  0.966816   \n",
       "11  18409261F5C2              Position    (151, 163)    126   137  0.897904   \n",
       "\n",
       "                                     predictionstring  \n",
       "0                 154 155 156 157 158 159 160 161 162  \n",
       "1             146 147 148 149 150 151 152 153 154 155  \n",
       "2                 138 139 140 141 142 143 144 145 146  \n",
       "3   972 973 974 975 976 977 978 979 980 981 982 98...  \n",
       "4   721 722 723 724 725 726 727 728 729 730 731 73...  \n",
       "5   424 425 426 427 428 429 430 431 432 433 434 43...  \n",
       "6   993 994 995 996 997 998 999 1000 1001 1002 100...  \n",
       "7   164 165 166 167 168 169 170 171 172 173 174 17...  \n",
       "8   465 466 467 468 469 470 471 472 473 474 475 47...  \n",
       "9   802 803 804 805 806 807 808 809 810 811 812 81...  \n",
       "10  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "11    126 127 128 129 130 131 132 133 134 135 136 137  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub['id']=='18409261F5C2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d66fd7",
   "metadata": {
    "papermill": {
     "duration": 0.036451,
     "end_time": "2022-03-15T13:48:25.045699",
     "exception": false,
     "start_time": "2022-03-15T13:48:25.009248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 464.896999,
   "end_time": "2022-03-15T13:48:26.395491",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-15T13:40:41.498492",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
